usa_dp_pos   <- plot_topic_daily_perc(usa_daily_topics_pos,  'USA'         ,topics)
spain_dp_pos <- plot_topic_daily_perc(spain_daily_topics_pos,'Spain'       ,topics,lab='y')
italy_dp_pos <- plot_topic_daily_perc(italy_daily_topics_pos,'Italy'       ,topics)
grman_dp_pos <- plot_topic_daily_perc(grman_daily_topics_pos,'Germany'     ,topics,lab='x')
uk_dp_neg    <- plot_topic_daily_perc(uk_daily_topics_neg,   'Negative\n'  ,topics)
usa_dp_neg   <- plot_topic_daily_perc(usa_daily_topics_neg,  ' '           ,topics)
spain_dp_neg <- plot_topic_daily_perc(spain_daily_topics_neg,' '           ,topics,leg = TRUE)
italy_dp_neg <- plot_topic_daily_perc(italy_daily_topics_neg,' '           ,topics)
grman_dp_neg <- plot_topic_daily_perc(grman_daily_topics_neg,' '           ,topics,lab='x')
((uk_dp_pos|uk_dp_neg) / (usa_dp_pos|usa_dp_neg) / (spain_dp_pos|spain_dp_neg) / (italy_dp_pos|italy_dp_neg) / (grman_dp_pos|grman_dp_neg)) + plot_layout(nrow = 5) & theme(legend.key.size = unit(0.25, 'cm'), legend.box.spacing = unit(0.1, 'cm'))
# Plot and save output
ggsave(paste0(rp,'fig\\topic_models\\daily_topics_perc_',topics,'_pn_exp.png'), width = 6.5, height = 6.5, dpi=300)
# Examine topic tweets
lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-01 00:00:00"), as.POSIXct("2020-03-7 00:00:00") )) %>%
arrange(-gamma)
# Set rootpaths
rp <- 'C:\\Users\\sgmmahon\\Documents\\GitHub\\iom_project\\'
dp <- 'data\\tweet_data\\'
mp <- 'methods\\'
# Function to read in data
read_in_csv <- function(x) {
x <- suppressWarnings(suppressMessages(read_csv(paste0(rp,dp,x))))
x$X1 <- NULL
return(x)
}
# Read in tweets
uk_tweets    <- read_in_csv('tweets\\uk_tweets_01122019_01052020_VLC.csv')
usa_tweets   <- read_in_csv('tweets\\usa_tweets_01122019_01052020_VLC.csv')
spain_tweets <- read_in_csv('tweets\\spain_tweets_01122019_01052020_VTLC.csv')
italy_tweets <- read_in_csv('tweets\\italy_tweets_01122019_01052020_VTLC.csv')
grman_tweets <- read_in_csv('tweets\\grman_tweets_01122019_01052020_VTLC.csv')
# Function to remove unwanted duplicated tweets
rm_dupl_rt <- function(x) {
# Extract original status_ids from all retweets
rsid <- x$retweeted_user %>%
sapply(function(x) regmatches(x,regexec("'status_id': \\s*(.*?)\\s*, 'u", x))[[1]][2] ) %>%
unname() %>% as.numeric()
# Create vector or status id's of tweets in dataset
sid <- x$status_id
# if statment conditional on whether a retweer and it's original are present in the data
if (length(which(rsid %in% sid)) > 0) { # If retweet and original are present
x <- x[!c(1:nrow(x)) %in% which(rsid %in% sid),] # Remove retweets where original tweet has already been captured
x <- x[is.na(x$retweeted_user) | !duplicated(x$retweeted_user),] # Remove duplicate retweets
} else { # If orignal and retweet have not been captured
x <- x[is.na(x$retweeted_user) | !duplicated(x$retweeted_user),] # Remove duplicate retweets
}
# Return results
return(x)
}
# Function to remove unwanted duplications in the data (e.g. retweets when the orginal has been captured)
rm_unwanted_dupl <- function(x) {
# Find tweets with duplicate text
dpls <- x[duplicated(x$VADER_text) | duplicated(x$VADER_text, fromLast = TRUE),]
# Split into individual dataframes
dpls_split <- dpls %>% split(.$VADER_text)
# Remove unwanted duplicates
dpls_usrnm <- lapply(dpls_split, function(y) rm_dupl_rt(y) )
# rbind dataframes back together then subset original dpls to only include tweets to be removed
dpls <- dpls[!dpls$status_id %in% do.call(rbind,dpls_usrnm)$status_id,]
# Removed these unwanted tweets from original dataframe
x <- x[!x$status_id %in% dpls$status_id,]
return(x)
}
# Remove unwanted duplicates from the data
uk_tweets    <- rm_unwanted_dupl(uk_tweets)
usa_tweets   <- rm_unwanted_dupl(usa_tweets)
# Function which creates a date variable,
# removes the index imported as part of the csv file and
# assigned a country id
format_data <- function(x) {
# Split string character date
date_df <- x$created_at %>%
str_split(" ", simplify = TRUE) %>%
as.data.frame()
# Create date variable
x$date <- ymd(date_df$V1)
# Remove original index and group column
x$X1          <- NULL
x$group_index <- NULL
# Determine country
cntry <- names(sort(table(x$country),decreasing=TRUE)[1])
# Assign country id
if        (cntry == "United Kingdom") {
x$cntry_id <- 1
} else if (cntry == "United States") {
x$cntry_id <- 2
} else if (cntry == "Spain") {
x$cntry_id <- 3
} else if (cntry == "Italy") {
x$cntry_id <- 4
} else {            #Germany
x$cntry_id <- 5
}
if ('translated_text' %in% colnames(x)) {
names(x)[names(x) == "translated_text"] <- "sentiment_text"
} else {
names(x)[names(x) == "VADER_text"] <- "sentiment_text"
}
# One tweet in the USA had a misallocated retweet count (was status_id). This was throwing errors, so is removed by this line
x <- subset(x,!is.na(suppressWarnings(as.integer(x$retweet_count))))
# Creating weightings variable, which accounts for retweets of certain tweets
# Default the weighting to 1
x$weightings <- 1
x$rownames <- as.numeric(row.names(x))
# Assign all retweets a weighting equivalent to their retweet count
nz_rts <- subset(x,x$retweet_count > 0)$rownames
x[x$rownames %in% nz_rts,'weightings'] <- x[x$rownames %in% nz_rts,'retweet_count']
# Assign additional tweet to all original tweets with a retweet_count > 0
or_twt <- subset(x,x$retweet_count > 0 & is.na(x$retweeted_user))$rownames
x[x$rownames %in% or_twt,'weightings'] <- x[x$rownames %in% or_twt,'retweet_count'] + 1
# Remove rownames
x$rownames <- NULL
# Return results
return(x)
}
# Format countries
uk_tweets    <- format_data(uk_tweets)
usa_tweets   <- format_data(usa_tweets)
spain_tweets <- format_data(spain_tweets)
italy_tweets <- format_data(italy_tweets)
grman_tweets <- format_data(grman_tweets)
# Read-in sentiment scores
uk_sent    <- read_in_csv('vader_sentiments\\uk_vader_sent_01122019_01052020.csv')
#uk_sent_2  <- read_in_csv('vader_sentiments\\uk_vader_sent_01052020_01112020.csv')
#uk_sent    <- rbind(uk_sent, uk_sent_2)
usa_sent   <- read_in_csv('vader_sentiments\\usa_vader_sent_01122019_01052020.csv')
#usa_sent_2 <- read_in_csv('vader_sentiments\\usa_vader_sent_01052020_01112020.csv')
#usa_sent   <- rbind(usa_sent, usa_sent_2)
spain_sent <- read_in_csv('vader_sentiments\\spain_vader_sent_01122019_01052020.csv')
italy_sent <- read_in_csv('vader_sentiments\\italy_vader_sent_01122019_01052020.csv')
grman_sent <- read_in_csv('vader_sentiments\\grman_vader_sent_01122019_01052020.csv')
# Function which assigns sentiment scores to tweets
summarise_data <- function(x,y) {
# Rename 'text' column in tweet data. Appears to interfere with objects in the functions used
names(x)[names(x) == "text"] <- "original_text"
# Join data
df <- cbind(x$date, y, x$created_at, x$status_id, x$weightings, x$cntry_id, x$lexicon_text, x$sentiment_text)
# Rename variables
df <- df %>% dplyr::rename(date = `x$date`,
over_tweet_sent = compound,
created_at = `x$created_at`,
status_id = `x$status_id`,
weightings = `x$weightings`,
cntry_id = `x$cntry_id`,
lexicon_text = `x$lexicon_text`,
sentiment_text = `x$sentiment_text`)
# Add variables that categories tweet sentiments
df <- df %>%
mutate(pn_sent =
case_when(
over_tweet_sent == 0 ~ 0,
over_tweet_sent > 0 ~ 1,
over_tweet_sent < 0 ~ 2
), # ID for neutral, positive and negative sentiment
pn5c_sent =
case_when(
over_tweet_sent >= -.05 & over_tweet_sent <= .05 ~ 3,
over_tweet_sent < -.5 ~ 1,
over_tweet_sent < -.05 & over_tweet_sent >= -.5 ~ 2,
over_tweet_sent > .05 & over_tweet_sent <= .5 ~ 4,
over_tweet_sent > .5 ~ 5,
)
)
}
# Compute summary metrics
uk_tweets    <- summarise_data(uk_tweets,data.frame(uk_sent))
usa_tweets   <- summarise_data(usa_tweets,data.frame(usa_sent))
spain_tweets <- summarise_data(spain_tweets,data.frame(spain_sent))
italy_tweets <- summarise_data(italy_tweets,data.frame(italy_sent))
grman_tweets <- summarise_data(grman_tweets,data.frame(grman_sent))
# Remove Trump's tweets about banning US migration from the German dataset (as it is causing a massive amount of bias)
trump_tweet <- grman_tweets %>%
filter(between(created_at, as.POSIXct("2020-04-21 00:00:00"), as.POSIXct("2020-04-22 00:00:00") )) %>%
arrange(-weightings) %>%
.[1,"status_id"]
grman_tweets <- grman_tweets %>% filter(status_id != trump_tweet)
# Consolidate all tweets into single dataframe (non-weighted)
tweets <- rbind(uk_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
usa_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
spain_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
italy_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
grman_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')])
uk_tweets
# Consolidate all tweets into single dataframe (non-weighted)
tweets <- rbind(uk_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
usa_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
spain_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
italy_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')],
grman_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text','original_text')])
names(tweets)
names(uk_tweets)
# Consolidate all tweets into single dataframe (non-weighted)
tweets <- rbind(uk_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text')],
usa_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text')],
spain_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text')],
italy_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text')],
grman_tweets[,c('date','created_at','status_id','weightings','over_tweet_sent',
'pn_sent','cntry_id','lexicon_text','sentiment_text')])
rm(uk_tweets,usa_tweets,spain_tweets,italy_tweets,grman_tweets)
# Removed reassign names of duplicated tweets (tweets which have been shared in more than 1 country).
dupl <- tweets$status_id[duplicated(tweets$status_id)]
tweets[tweets$status_id %in% dupl,'status_id'] <- seq(1,nrow(tweets[tweets$status_id %in% dupl,]),1)
# Assign unique status ids to each tweet, so they are not regected by the lexicon model
#tweets$status_id <- c(1:nrow(tweets))
# Remove tweets that have no terms in them after Python text cleaning
tweets <- subset(tweets, !is.na(tweets$lexicon_text))
# Hold date for later
timestamp <- tweets[,c("status_id","created_at",'weightings','over_tweet_sent','pn_sent',"cntry_id",'sentiment_text','original_text')]
# Hold date for later
timestamp <- tweets[,c("status_id","created_at",'weightings','over_tweet_sent','pn_sent',"cntry_id",'sentiment_text')]
# Aggregate to take highest probabilty for topic
lda_agg <- lda_docs %>%
group_by(document) %>%
filter(gamma == max(gamma))
# Join back on timestamp
timestamp$status_id <- as.character(timestamp$status_id) # To match variable type
lda_agg <- merge(lda_agg, timestamp, by.x = "document", by.y = "status_id", all.x = TRUE)
# Define English Search Terms
neutral_migrant_terms   = c("immigrant", "immigration", "migrant", "migration", "\"asylum seeker\"", "refugee", "\"undocumented worker\"",
"\"guest worker\"", "\"EU worker\"", "\"non-UK workers\"", "\"foreign worker\"", "(human smuggling)", "(human trafficking)")
negative_migrant_terms  = c("illegals", "foreigner", "\"illegal alien\"", "\"illegal worker\"")
negative_racial_terms   = c("islamophob*", "sinophob*", "\"china flu\"", "\"kung flu\"", "\"china virus\"", "\"chinese virus\"", "shangainese")
search_terms <- c(neutral_migrant_terms, negative_migrant_terms, negative_racial_terms) %>% paste(collapse = " | ")
# For k = 12
# Filter topic 7 to only include migrant terms
#MED <- lda_agg %>% filter(topic == 7) %>% filter(grepl(search_terms, sentiment_text)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
#lda_agg[lda_agg$document %in% MED$document & lda_agg$topic == 7,]$topic <- 13
#12 "Anti-Racism"               1
#6  "Pro-Migrant Activism"      2
#2  "Human Rights Abuses"       3
#1  "EU Refugee Crisis"         4
#3  "Italian Migrant Debate"    5
#4  "US Migrant Debate"         6
#9  "Brexit"                    7
#13 "Migration & Econ/Demogr"   8
#10 "Coronavirus"               9
#8  "Academic & Public Events"  10
#7  "Law & Employment"          11
#11 "Weather/Drinks (#ICE)"     11
#5  "Miscellaneous"             11
# Re-categories topics
#lda_agg$topic_new <- lda_agg$topic + 20 # Create new topic variable that doesn't interfere with existing topics (topic + 20)
#lda_agg[lda_agg$topic_new == (20 + 12),]$topic <- 1  # Topic 12 to 1
#lda_agg[lda_agg$topic_new == (20 +  6),]$topic <- 2  # Topic 6  to 2
#lda_agg[lda_agg$topic_new == (20 +  2),]$topic <- 3  # Topic 2  to 3
#lda_agg[lda_agg$topic_new == (20 +  1),]$topic <- 4  # Topic 1  to 4
#lda_agg[lda_agg$topic_new == (20 +  3),]$topic <- 5  # Topic 3  to 5
#lda_agg[lda_agg$topic_new == (20 +  4),]$topic <- 6  # Topic 4  to 6
#lda_agg[lda_agg$topic_new == (20 +  9),]$topic <- 7  # Topic 9  to 7
#lda_agg[lda_agg$topic_new == (20 + 13),]$topic <- 8  # Topic 13 to 8
#lda_agg[lda_agg$topic_new == (20 + 10),]$topic <- 9  # Topic 10 to 9
#lda_agg[lda_agg$topic_new == (20 +  8),]$topic <- 10 # Topic 8  to 10
#lda_agg[lda_agg$topic_new == (20 +  7),]$topic <- 11 # Topic 7  to 11
#lda_agg[lda_agg$topic_new == (20 + 11),]$topic <- 11 # Topic 11 to 11
#lda_agg[lda_agg$topic_new == (20 +  5),]$topic <- 11 # Topic 5  to 11
#lda_agg$topic_new <- NULL # Remove new topic variable
# Redefine number of topics used
#topics <- 11
# For k = 11
# Filter topic 7 to only include migrant terms
#temp <- lda_agg %>% filter(topic == 6) %>% filter(grepl(search_terms, sentiment_text)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
#lda_agg[lda_agg$document %in% MED$document & lda_agg$topic == 7,]$topic <- 12
#1  "Right-wing/Anti-Muslim Extremism"      1
#2  "Political Discontentment"              2
#3  "Migrant Boat Crossings"                3
#4  "Academic & Public Events"              4
#5  "Discrimination & Xenophobia" # Filter? 5
#6  "Legal & Employment Issues"   # Filter? 10
#7  "Coronavirus"                           6
#8  "Weather (#ICE)"                        10
#9  "Brexit & Immigration Systems"          7
#10 "Human Rights Abuses"                   8
#11 "Trump / US Border"                     9
# Re-categories topics
#lda_agg$topic_new <- lda_agg$topic + 20 # Create new topic variable that doesn't interfere with existing topics (topic + 20)
#lda_agg[lda_agg$topic_new == (20 +  1),]$topic <- 1  # Topic 1  to 1
#lda_agg[lda_agg$topic_new == (20 +  2),]$topic <- 2  # Topic 2  to 2
#lda_agg[lda_agg$topic_new == (20 +  3),]$topic <- 3  # Topic 3  to 3
#lda_agg[lda_agg$topic_new == (20 +  4),]$topic <- 4  # Topic 4  to 4
#lda_agg[lda_agg$topic_new == (20 +  5),]$topic <- 5  # Topic 5  to 5
#lda_agg[lda_agg$topic_new == (20 +  7),]$topic <- 6  # Topic 4  to 6
#lda_agg[lda_agg$topic_new == (20 +  9),]$topic <- 7  # Topic 9  to 7
#lda_agg[lda_agg$topic_new == (20 + 10),]$topic <- 8  # Topic 13 to 8
#lda_agg[lda_agg$topic_new == (20 + 11),]$topic <- 9  # Topic 10 to 9
#lda_agg[lda_agg$topic_new == (20 +  6),]$topic <- 10 # Topic 8  to 10
#lda_agg[lda_agg$topic_new == (20 +  8),]$topic <- 10 # Topic 7  to 11
#lda_agg$topic_new <- NULL # Remove new topic variable
# Redefine number of topics used
#topics <- 10
# For k = 14
#1  "Migration & Econ/Demogr"       1
#2  "Migrant Boat Crossings"        2
#3  "Weather/Drinks/Cryo (#ICE)"    11
#4  "Miscellaneous"                 11
#5  "Brexit"                        3
#6  "Business/Law/Data"             11
#7  "Anti-\'Right-Wing Extremism\'" 4
#8  "Misc/Migration Reform"         11
#9  "Coronavirus"                   5
#10 "US Detention Centres"          6
#11 "Racism & Xenophobia"           7
#12 "EU Refugee Crisis"             8
#13 "Trump / Illegal Immigration"   9
#14 "Human Rights Abuses"           10
#15 "Misc/Migration Reform" (Filt)  9
# Filter topic 8 to only include migrant terms
#trmp_ill_migr <- lda_agg %>% filter(topic == 8) %>% filter(grepl(search_terms, sentiment_text)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
#lda_agg[lda_agg$document %in% trmp_ill_migr$document & lda_agg$topic == 8,]$topic <- 10
# Re-categories topics
#lda_agg$topic_new <- lda_agg$topic + 20 # Create new topic variable that doesn't interfere with existing topics (topic + 20)
#lda_agg[lda_agg$topic_new == (20 +  1),]$topic <- 1  # Topic 1  to 1
#lda_agg[lda_agg$topic_new == (20 +  2),]$topic <- 2  # Topic 2  to 2
#lda_agg[lda_agg$topic_new == (20 +  3),]$topic <- 11 # Topic 3  to 11
#lda_agg[lda_agg$topic_new == (20 +  4),]$topic <- 11 # Topic 4  to 11
#lda_agg[lda_agg$topic_new == (20 +  5),]$topic <- 3  # Topic 5  to 3
#lda_agg[lda_agg$topic_new == (20 +  6),]$topic <- 11 # Topic 6  to 11
#lda_agg[lda_agg$topic_new == (20 +  7),]$topic <- 4  # Topic 7  to 4
#lda_agg[lda_agg$topic_new == (20 +  8),]$topic <- 11 # Topic 8  to 11
#lda_agg[lda_agg$topic_new == (20 +  9),]$topic <- 5  # Topic 9  to 5
#lda_agg[lda_agg$topic_new == (20 + 10),]$topic <- 6  # Topic 10 to 6
#lda_agg[lda_agg$topic_new == (20 + 11),]$topic <- 7  # Topic 11 to 7
#lda_agg[lda_agg$topic_new == (20 + 12),]$topic <- 8  # Topic 12 to 8
#lda_agg[lda_agg$topic_new == (20 + 13),]$topic <- 9  # Topic 13 to 9
#lda_agg[lda_agg$topic_new == (20 + 14),]$topic <- 10 # Topic 14 to 10
#lda_agg$topic_new <- NULL # Remove new topic variable
# Redefine number of topics used
#topics <- 11
# For k = 15
# Commercial Law
# Legal Assistance - filter commecial terms out of topic
temp <- lda_agg %>% filter(topic == 3) %>% filter(!grepl("lawyer|attorney|client|AlexHanna|firm|company|companies|corporate|commercial",
sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 3,]$topic <- 16
# Brexit - filter brexit terms from topic 6
search_terms <- c("brexit", "nigel", "farage", neutral_migrant_terms, negative_migrant_terms, negative_racial_terms) %>% paste(collapse = "|")
temp <- lda_agg %>% filter(topic == 6) %>% filter(grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 6,]$topic <- 17
# Academic & Public Events - filter out non-data/nature events from topic 9
search_terms <- c(neutral_migrant_terms, negative_migrant_terms, negative_racial_terms) %>% paste(collapse = "|")
filter_terms <- c("data","automate","wordpress","sterling","4HANA","nature","ecology","website","OAuth","authentication",
"offers","savings","seamless","users","full-stack","4g","5g","technology","purchase","windows",
"software","backend","developer") %>% paste(collapse = "|")
temp <- lda_agg %>% filter(topic == 9) %>% filter(grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>%
filter(!grepl(filter_terms, sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 9,]$topic <- 18
# Misc Migration - filter out migration terms from topic 12
search_terms <- c(neutral_migrant_terms, negative_migrant_terms, negative_racial_terms) %>% paste(collapse = "|")
temp <- lda_agg %>% filter(topic == 12) %>% filter(grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 12,]$topic <- 19
# Misc Migration Travel - filter out migration terms from topic 13
search_terms <- c("cage", "detention", "detain", "holding facility", "ICE",
neutral_migrant_terms, negative_migrant_terms, negative_racial_terms) %>% paste(collapse = "|")
temp <- lda_agg %>% filter(topic == 13) %>% filter(grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 13,]$topic <- 20
# EU Refugee Crisis 2 - filter out migrant and refugee terms (but not ICE) from topic 15
search_terms <- c("camp","Moria","resettle","#WirHabenPlatz","evacuat","safe haven","greek","greece",
neutral_migrant_terms, negative_migrant_terms, negative_racial_terms) %>% paste(collapse = "|")
temp <- lda_agg %>% filter(topic == 15) %>% filter(grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>%
filter(!grepl("ICE|Trump", sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 15,]$topic <- 21
# Coronavirus 2 - filter out covid terms from topic 15
temp <- lda_agg %>% filter(topic == 15) %>% filter(!grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>%
filter(grepl("corona|covid|distancing|StayAtHome|wash|medical", sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 15,]$topic <- 22
# Human Rights Abuses 2 - filter out migrant detention terms from topic 15
temp <- lda_agg %>% filter(topic == 15) %>% filter(!grepl(search_terms, sentiment_text, ignore.case = TRUE)) %>%
filter(grepl("cage|detention|detain|holding facility|ICE|Trump", sentiment_text, ignore.case = TRUE)) %>% arrange(-gamma)
# Reassign these tweets a new topic (there was a lot of noise in this category, so the other tweets will be assigned neutral)
lda_agg[lda_agg$document %in% temp$document & lda_agg$topic == 15,]$topic <- 23
#1  Miscellaneous            #12
#2  Coronavirus              #6
#3  Commercial Law           #12
#4  Migrant Boat Crossings   #10
#5  Illegal Immigration      #8
#6  Misc UK                  #12
#7  Trump                    #3
#8  Racism / Xenophobia      #7
#9  Misc Events              #12
#10 Weather                  #12
#11 Vulnerable EU Migrants   #4
#12 Miscellaneous            #12
#13 Misc Travel              #12
#14 Human Rights Abuses      #1
#15 Activism                 #2
#16 Legal Assistance         #11
#17 Brexit                   #9
#18 Academic & Public Events #5
#19 Misc Migration           #11
#20 Misc Migration Travel    #11
#21 Vulnerable EU Migrants 2 #4
#22 Coronavirus 2            #6
#23 Human Rights Abuses 2    #1
# Re-categories topics
lda_agg$topic_new <- lda_agg$topic + 30 # Create new topic variable that doesn't interfere with existing topics (topic + 20)
lda_agg[lda_agg$topic_new == (30 +  1),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 +  2),]$topic <- 6
lda_agg[lda_agg$topic_new == (30 +  3),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 +  4),]$topic <- 10
lda_agg[lda_agg$topic_new == (30 +  5),]$topic <- 8
lda_agg[lda_agg$topic_new == (30 +  6),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 +  7),]$topic <- 3
lda_agg[lda_agg$topic_new == (30 +  8),]$topic <- 7
lda_agg[lda_agg$topic_new == (30 +  9),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 + 10),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 + 11),]$topic <- 4
lda_agg[lda_agg$topic_new == (30 + 12),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 + 13),]$topic <- 12
lda_agg[lda_agg$topic_new == (30 + 14),]$topic <- 1
lda_agg[lda_agg$topic_new == (30 + 15),]$topic <- 2
lda_agg[lda_agg$topic_new == (30 + 16),]$topic <- 11
lda_agg[lda_agg$topic_new == (30 + 17),]$topic <- 9
lda_agg[lda_agg$topic_new == (30 + 18),]$topic <- 5
lda_agg[lda_agg$topic_new == (30 + 19),]$topic <- 11
lda_agg[lda_agg$topic_new == (30 + 20),]$topic <- 11
lda_agg[lda_agg$topic_new == (30 + 21),]$topic <- 4
lda_agg[lda_agg$topic_new == (30 + 22),]$topic <- 6
lda_agg[lda_agg$topic_new == (30 + 23),]$topic <- 1
lda_agg$topic_new <- NULL # Remove new topic variable
# Redefine number of topics used
topics <- 12
lda_agg2 <- lda_agg
# Examine topic tweets
lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-01 00:00:00"), as.POSIXct("2020-03-7 00:00:00") )) %>%
arrange(-gamma)
# Examine topic tweets
pos <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-01 00:00:00"), as.POSIXct("2020-03-7 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-gamma)
neg <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-01 00:00:00"), as.POSIXct("2020-03-7 00:00:00") )) %>%
filter(over_tweet_sent < 0) %>% arrange(-gamma)
pos$over_tweet_sent
mean(pos$over_tweet_sent)
mean(neg$over_tweet_sent)
# Examine topic tweets
pos1 <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-01 00:00:00"), as.POSIXct("2020-03-7 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-gamma)
# Examine topic tweets
pos2 <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-08 00:00:00"), as.POSIXct("2020-03-14 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-gamma)
# Examine topic tweets
pos3 <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-15 00:00:00"), as.POSIXct("2020-03-21 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-gamma)
neg1 <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-01 00:00:00"), as.POSIXct("2020-03-7 00:00:00") )) %>%
filter(over_tweet_sent < 0) %>% arrange(-gamma)
neg2 <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-08 00:00:00"), as.POSIXct("2020-03-14 00:00:00") )) %>%
filter(over_tweet_sent < 0) %>% arrange(-gamma)
neg3 <- lda_agg2 %>% filter(topic == 4, cntry_id == 5) %>%
filter(between(created_at, as.POSIXct("2020-03-15 00:00:00"), as.POSIXct("2020-03-21 00:00:00") )) %>%
filter(over_tweet_sent < 0) %>% arrange(-gamma)
mean(neg1$over_tweet_sent)
mean(neg2$over_tweet_sent)
mean(neg3$over_tweet_sent)
mean(pos1$over_tweet_sent)
mean(pos2$over_tweet_sent)
mean(pos3$over_tweet_sent)
# Examine topic tweets
pos3 <- lda_agg2 %>% filter(topic == 3, cntry_id == 1) %>%
filter(between(created_at, as.POSIXct("2020-04-21 00:00:00"), as.POSIXct("2020-04-30 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-gamma)
# Examine topic tweets
lda_agg2 %>% filter(topic == 3, cntry_id == 1) %>%
filter(between(created_at, as.POSIXct("2020-04-21 00:00:00"), as.POSIXct("2020-04-30 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-gamma)
# Examine topic tweets
lda_agg2 %>% filter(topic == 3, cntry_id == 1) %>%
filter(between(created_at, as.POSIXct("2020-04-21 00:00:00"), as.POSIXct("2020-04-30 00:00:00") )) %>%
filter(over_tweet_sent > 0) %>% arrange(-weightings)
lda_agg2 %>% filter(topic == 3, cntry_id == 2) %>%
filter(between(created_at, as.POSIXct("2020-04-21 00:00:00"), as.POSIXct("2020-04-30 00:00:00") )) %>%
filter(over_tweet_sent < 0) %>% arrange(-weightings)
# Import libraries
library(tidyverse)
library(kableExtra)
library(stringi)
library(flextable)
library(magrittr)
library(R.utils)
library(rmarkdown)
library(webshot)
library(RColorBrewer)
