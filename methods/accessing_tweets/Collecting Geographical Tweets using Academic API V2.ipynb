{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "# To add wait time between requests\n",
    "import time\n",
    "# For matching string expressions\n",
    "import re\n",
    "# For generating summary statistics\n",
    "import statistics as st\n",
    "\n",
    "# Define rootpath\n",
    "rp = 'C:\\\\Users\\\\sgmmahon\\\\Documents\\\\GitHub\\\\iom_project\\\\'\n",
    "mp = 'methods\\\\accessing_tweets\\\\'\n",
    "dp = 'data\\\\tweet_data\\\\' \n",
    "\n",
    "# Add Bearer Token as environmental variable\n",
    "#os.environ['TOKEN'] = 'BEARER-TOKEN-HERE'\n",
    "os.environ['TOKEN'] = \"AAAAAAAAAAAAAAAAAAAAABLt8wAAAAAAbE8U4tJJtQffIvLTtuHWVzm0nqI%3DIinQPSHHGhYjXwG71L4AHkUnUmlP9X8npfKQ0S6Nk07nBSF8AY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions which concatenates vectors\n",
    "def cnct (x): return(\" OR \".join(x))\n",
    "def cnctwb (x): return(\"(\" + \" OR \".join(x) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://1000mostcommonwords.com/1000-most-common-ukrainian-words/\n",
    "\n",
    "ua_common_words = [\"як\", \"Я\", \"його\", \"що\", \"він\", \"було\", \"для\", \"на\", \"є\", \"еякі\", \"вони\", \"бути\", \"у\", \"один\", \"мати\", \"це\", \n",
    "                   \"від\", \"по\", \"гаряча\", \"слово\", \"але\", \"що\", \"деякі\", \"вогонь\", \"це\", \"ви\", \"або\", \"було\", \"план\", \"и\", \"до\", \n",
    "                   \"і\", \"кішка\", \"в\", \"ми\", \"може\", \"чере\", \"другий\", \"були\", \"які\", \"зробити\", \"їх\", \"час\", \"якщо\", \"буде\", \"як\", \n",
    "                   \"аначений\", \"вона\", \"кожен\", \"скаати\", \"робить\", \"набір\", \"три\", \"хотіти\", \"повітря\", \"добре\", \"також\", \"грати\",\n",
    "                   \"невеликої\", \"кінець\", \"ставити\", \"додому\", \"читати\", \"рука\", \"порт\", \"великий\", \"аклинань\", \"додавати\", \n",
    "                   \"навіть\", \"емля\", \"тут\", \"повинні\", \"великий\", \"високий\", \"таких\", \"слідувати\", \"акт\", \"чому\", \"спитаєте\", \n",
    "                   \"чоловіки\", \"мінення\", \"пішов\", \"світло\", \"вид\", \"від\", \"потрібно\", \"будинок\", \"картинка\", \"спробуйте\", \"нам\", \n",
    "                   \"ову\", \"тварин\", \"точка\", \"мать\", \"світ\", \"рядом\", \"будувати\", \"самостійно\", \"емля\", \"батько\"]\n",
    "\n",
    "ua_common_words = cnctwb(ua_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.openrussian.org/list/all\n",
    "\n",
    "ru_common_words = ['и', 'в', 'не', 'на', 'что', 'тот', 'быт', 'с', 'а', 'весь', 'как', 'по', 'но', 'э́то', 'к', 'у', 'из', \n",
    "                'за', 'так', 'же', 'сказа́ть', 'э́тот', 'кото́рый', 'мочь', 'о', 'челове́к', 'ещё', 'бы', 'тако́й', 'то́лько', \n",
    "                'себя́', 'како́й', 'для', 'уже́', 'когда́', 'кто', 'вот', 'да', 'год', 'знать', 'е́сли', 'до', 'говори́ть', 'и́ли', \n",
    "                'мой', 'вре́мя', 'рука́', 'са́мый', 'нет', 'ни', 'стать', 'большо́й', 'друго́й', 'свой', 'де́ло', 'под', 'где', \n",
    "                'что́бы', 'ну', 'сам', 'есть', 'раз', 'чём', 'там', 'глаз', 'пе́рвый', 'день', 'жизнь', 'тут', 'ничто́', \n",
    "                'пото́м', 'о́чень', 'ли', 'при', 'хоте́ть', 'на́до', 'голова́', 'без', 'ви́деть', 'тепе́рь', 'идти́', 'друг', 'сейча́с', \n",
    "                'стоя́ть', 'дом', 'то́же', 'по́сле', 'мо́жно', 'сло́во', 'че́рез', 'ме́сто', 'ду́мать', 'здесь', 'спроси́ть', 'лицо́', \n",
    "                'тогда́', 'до́лжный', 'ведь', 'но́вый', 'ка́ждый']\n",
    "\n",
    "ru_common_words = cnctwb(ru_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function which retrieves token from the environment\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "# Define function that uses bearer token to create headers used to access the API\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "# Function to create url to make GET request\n",
    "def create_url(query, start_time, end_time, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {# Required parameters\n",
    "                    'query': query,\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'max_results': max_results,\n",
    "                    # Additional parameters which can be requested optionally\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id,geo.place_id,entities.mentions.username',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities,possibly_sensitive',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified,location,url',\n",
    "                    'place.fields': 'full_name,id,country,country_code,contained_within,geo,name,place_type',\n",
    "                    # ID to call next page of tweets\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "# Function to make GET request\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "# Function to obtain variable names\n",
    "def get_var_name(variable):\n",
    " for name in globals():\n",
    "     if eval(name) == variable:\n",
    "        return(name)\n",
    "\n",
    "\n",
    "# Function to append results to a csv\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    # A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    # Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    # If at least one tweet called contains geographic information\n",
    "    if 'places' in json_response['includes']:\n",
    "        # Create a dataframe of places called within this batch of tweets\n",
    "        places = pd.DataFrame(columns=['place_id','place_name','full_place_name','bbox','lat','long','exact_coords','place_type','country_code','country'])\n",
    "        # For loop which appends information about each place to the places dataframe \n",
    "        for place in json_response['includes']['places']:\n",
    "            # Collate all place information as dictionary\n",
    "            place_data = {'place_id'       :place['id'          ],\n",
    "                          'place_name'     :place['name'        ],\n",
    "                          'full_place_name':place['full_name'   ],\n",
    "                          'bbox'           :[' '.join(str(coord) for coord in place['geo']['bbox' ])],\n",
    "                          'lat'            :st.mean([place['geo']['bbox'][0],place['geo']['bbox'][2]]),\n",
    "                          'long'           :st.mean([place['geo']['bbox'][1],place['geo']['bbox'][3]]),\n",
    "                          'exact_coords'   :False,\n",
    "                          'place_type'     :place['place_type'  ],\n",
    "                          'country_code'   :place['country_code'],\n",
    "                          'country'        :place['country'     ]}\n",
    "        \n",
    "            # Convert dictionary to a single-row dataframe\n",
    "            place_data = pd.DataFrame(place_data)\n",
    "            # Append place information to the places dataframe\n",
    "            places = places.append(place_data)\n",
    "    \n",
    "        # Drop duplicate places and reset index of places\n",
    "        places = places.drop_duplicates(subset=['place_id']).reset_index().drop('index',axis=1)\n",
    "    \n",
    "    # Create a dataframe of users mentioned in this batch of tweets\n",
    "    users = pd.DataFrame(columns=['user_id','username','user_name','followers_count','following_count',\n",
    "                                  'tweet_count','listed_count','user_url','user_loc','user_desc'])\n",
    "    \n",
    "    # For loop which appends information about each user to the users dataframe\n",
    "    for user in json_response['includes']['users']:\n",
    "        # Collate all user information as dictionary\n",
    "        user_data = {'user_id'        :user['id'         ],\n",
    "                     'username'       :user['username'   ],\n",
    "                     'user_name'      :user['name'       ],\n",
    "                     'user_url'       :user['url'        ],\n",
    "                     'user_desc'      :user['description'],\n",
    "                     'followers_count':user['public_metrics']['followers_count'],\n",
    "                     'following_count':user['public_metrics']['following_count'],\n",
    "                     'tweet_count'    :user['public_metrics']['tweet_count'    ],\n",
    "                     'listed_count'   :user['public_metrics']['listed_count'   ]}\n",
    "    \n",
    "        if 'location' in user:\n",
    "            user_data.update({'user_loc': [user['location']]})\n",
    "        else:\n",
    "            user_data.update({'user_loc': [' ']})\n",
    "        \n",
    "        # Convert dictionary to a single-row dataframe\n",
    "        user_data = pd.DataFrame(user_data)\n",
    "        # Append place information to the places dataframe\n",
    "        users = users.append(user_data)\n",
    "    \n",
    "    # Drop duplicate entries and reset index of users\n",
    "    users = users.drop_duplicates(subset=['user_id']).reset_index().drop('index',axis=1)\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "        \n",
    "        # 4. Geolocation\n",
    "        \n",
    "        # If tweet contains geographical information\n",
    "        if ('geo' in tweet):\n",
    "            \n",
    "            # Subset places dataframe to only include information about place mentioned in tweet\n",
    "            place = places[places['place_id'] == tweet['geo']['place_id']]\n",
    "            \n",
    "            # Assign geographical variables unsing place information\n",
    "            place_id        = tweet['geo']['place_id']\n",
    "            place_name      = place.loc[place.index[0],'place_name']\n",
    "            full_place_name = place.loc[place.index[0],'full_place_name']\n",
    "            bbox            = place.loc[place.index[0],'bbox']\n",
    "            place_type      = place.loc[place.index[0],'place_type']\n",
    "            country_code    = place.loc[place.index[0],'country_code']\n",
    "            country         = place.loc[place.index[0],'country']\n",
    "            \n",
    "            # If tweet contains exact coordinates, provide them and assign exact_coords as True\n",
    "            if ('coordinates' in tweet['geo']):\n",
    "                lat             = tweet['geo']['coordinates']['coordinates'][0]\n",
    "                long            = tweet['geo']['coordinates']['coordinates'][1]\n",
    "                exact_coords    = True\n",
    "            # If tweet doesn't contains exact coordinates, provide centre of place bounding box and assign exact_coords as False\n",
    "            else:\n",
    "                lat             = place.loc[place.index[0],'lat']\n",
    "                long            = place.loc[place.index[0],'long']\n",
    "                exact_coords    = False\n",
    "        \n",
    "        # If no geographical information provided, assign all geographical variables as blank\n",
    "        else:\n",
    "            place_id        = \" \"\n",
    "            place_name      = \" \"\n",
    "            full_place_name = \" \"\n",
    "            bbox            = \" \"\n",
    "            place_type      = \" \"\n",
    "            country_code    = \" \"\n",
    "            country         = \" \"\n",
    "            lat             = \" \"\n",
    "            long            = \" \"\n",
    "            exact_coords    = \" \"\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count   = tweet['public_metrics']['reply_count']\n",
    "        like_count    = tweet['public_metrics']['like_count']\n",
    "        quote_count   = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        # 7. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # 8. Users\n",
    "        \n",
    "        # Subset users dataframe to only include information about user mentioned who tweeted\n",
    "        user = users[users['user_id'] == tweet['author_id']]\n",
    "        \n",
    "        # Assign geographical variables unsing place information\n",
    "        username        = user.loc[user.index[0],'username']\n",
    "        user_name       = user.loc[user.index[0],'user_name']\n",
    "        followers_count = user.loc[user.index[0],'followers_count']\n",
    "        following_count = user.loc[user.index[0],'following_count']\n",
    "        tweet_count     = user.loc[user.index[0],'tweet_count']\n",
    "        listed_count    = user.loc[user.index[0],'listed_count']\n",
    "        user_url        = user.loc[user.index[0],'user_url']\n",
    "        user_loc        = user.loc[user.index[0],'user_loc']\n",
    "        user_desc       = user.loc[user.index[0],'user_desc']\n",
    "        \n",
    "        # 9. Source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 10. Conversation_id\n",
    "        conversation_id = tweet['conversation_id']\n",
    "\n",
    "        # 11. Reply settings\n",
    "        reply_settings = tweet['reply_settings']\n",
    "\n",
    "        # 12. Referenced tweets\n",
    "        if ('referenced_tweets' in tweet):   \n",
    "            referenced_tweets_type = tweet['referenced_tweets'][0]['type']\n",
    "            referenced_tweets_id   = tweet['referenced_tweets'][0]['id']\n",
    "        else:\n",
    "            referenced_tweets_type = \" \"\n",
    "            referenced_tweets_id   = \" \"\n",
    "\n",
    "        # 13. In reply to user id\n",
    "        if ('in_reply_to_user_id' in tweet):  \n",
    "            in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id = \" \"\n",
    "            \n",
    "        # 14. Entities\n",
    "        if ('entities' in tweet and 'annotations' in tweet['entities']):\n",
    "            annotations = tweet['entities']['annotations']\n",
    "        else:\n",
    "            annotations = \" \"\n",
    "        \n",
    "        # 15. Mentions\n",
    "        if ('entities' in tweet and 'mentions' in tweet['entities']):\n",
    "            mentions = tweet['entities']['mentions']\n",
    "        else:\n",
    "            mentions = \" \"\n",
    "        \n",
    "        # 16. URLs\n",
    "        if ('entities' in tweet and 'urls' in tweet['entities']):\n",
    "            linked_url = tweet['entities']['urls'][0]['expanded_url']\n",
    "        else:\n",
    "            linked_url = \" \"\n",
    "            \n",
    "        # 17. Possibly sensitive\n",
    "        if('possibly_sensitive' in tweet):\n",
    "            possibly_sensitive = tweet['possibly_sensitive']\n",
    "        else:\n",
    "            possibly_sensitive = \" \"\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [tweet_id, created_at, author_id, \n",
    "               place_id, place_name, full_place_name, lat, long, exact_coords, bbox, place_type, country_code, country,\n",
    "               lang, retweet_count, reply_count, like_count, quote_count, text, \n",
    "               username, user_name, followers_count, following_count, tweet_count, listed_count, user_url, user_loc, user_desc,\n",
    "               source, conversation_id, reply_settings, referenced_tweets_type, referenced_tweets_id, in_reply_to_user_id, \n",
    "               annotations, mentions, linked_url, possibly_sensitive]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)\n",
    "\n",
    "\n",
    "# Function that uses looping to make multiple calls, to ensure the desired number of tweets during each period have been collected \n",
    "def call_tweets(keywords,start_list,end_list,max_results,max_count,pathway):\n",
    "    \n",
    "    #Inputs for tweets\n",
    "    bearer_token = auth()\n",
    "    headers = create_headers(bearer_token)\n",
    "\n",
    "    #Total number of tweets we collected from the loop\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    csvFile = open(pathway, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    # Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['tweet_id', 'created_at', 'author_id', \n",
    "                        'place_id', 'place_name', 'full_place_name', 'lat', 'long', 'exact_coords', 'bbox', 'place_type', 'country_code', 'country', \n",
    "                        'lang', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'text', \n",
    "                        'username', 'user_name', 'followers_count', 'following_count', 'tweet_count', 'listed_count', 'user_url', 'user_loc', 'user_desc', \n",
    "                        'source', 'conversation_id', 'reply_settings', 'referenced_tweets_type', 'referenced_tweets_id', 'in_reply_to_user_id', \n",
    "                        'annotations', 'mentions', 'linked_url', 'possibly_sensitive'])\n",
    "    csvFile.close()\n",
    "    \n",
    "    # For loop which calls tweets until the max number per period (max_results) has been reached\n",
    "    for i in range(0,len(start_list)):\n",
    "        \n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            \n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            \n",
    "            # Call tweets\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(keywords, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            \n",
    "            # Get number of tweets called\n",
    "            result_count = json_response['meta']['result_count']\n",
    "            \n",
    "            # If 'next_token' provided\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                # Print next_token\n",
    "                print(\"Next Token: \", next_token)\n",
    "                # If results have been successfully called and additional results are ready\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    # Append results to csv and print progress\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, pathway)\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                # If results have been returned\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, pathway, keyword)\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)\n",
    "            \n",
    "                # Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(5)\n",
    "                \n",
    "    # Print total number of tweets called            \n",
    "    print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key elements of twitter V2 API call\n",
    "#json_response['data']               # Info about tweets\n",
    "#json_response['includes']['users']  # Info about users mentioned in tweets\n",
    "#json_response['includes']['places'] # Info about places attached to tweets\n",
    "#json_response['includes']['tweets'] # Info about tweets that interact with primary tweets (e.g. replies, quotes)\n",
    "#json_response['errors']             # Errors that occured when calling tweets\n",
    "#json_response['meta']               # Meta data like newest and oldest tweets, total tweets called and next token if paginating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Ukraine tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj6oycva7db6725ffk4ty6i44m0t\n",
      "Start Date:  2021-07-1T01:00:00.000Z\n",
      "# of Tweets added from this response:  491\n",
      "Total # of Tweets added:  491\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj6p6ukstwd3k4y1m7h0u3gtcnst\n",
      "Start Date:  2021-07-2T01:00:00.000Z\n",
      "# of Tweets added from this response:  484\n",
      "Total # of Tweets added:  975\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj73znkgncgegeowb1exjj2bl5z1\n",
      "Start Date:  2021-07-3T01:00:00.000Z\n",
      "# of Tweets added from this response:  497\n",
      "Total # of Tweets added:  1472\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj7461qqj10njziqm1bdjbyzdzzx\n",
      "Start Date:  2021-07-4T01:00:00.000Z\n",
      "# of Tweets added from this response:  497\n",
      "Total # of Tweets added:  1969\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj74ekybvkqxfl6gyb8mvvwj4ad9\n",
      "Start Date:  2021-07-5T01:00:00.000Z\n",
      "# of Tweets added from this response:  498\n",
      "Total # of Tweets added:  2467\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj7j5blv4n5g4bvflcjz18tmpbwd\n",
      "Start Date:  2021-07-6T01:00:00.000Z\n",
      "# of Tweets added from this response:  499\n",
      "Total # of Tweets added:  2966\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj7jdtlx5d4rz7l3gfuxk5t0obct\n",
      "Start Date:  2021-07-7T01:00:00.000Z\n",
      "# of Tweets added from this response:  500\n",
      "Total # of Tweets added:  3466\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj7y6lp4ly5ax3xtxxdh7dqcra7x\n",
      "Start Date:  2021-07-8T01:00:00.000Z\n",
      "# of Tweets added from this response:  497\n",
      "Total # of Tweets added:  3963\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj7yd29ogcwcb5dlsoqqbbbujgjh\n",
      "Start Date:  2021-07-9T01:00:00.000Z\n",
      "# of Tweets added from this response:  498\n",
      "Total # of Tweets added:  4461\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj7yljz8mcrcqmwfu162chlz2cn1\n",
      "Start Date:  2021-07-10T01:00:00.000Z\n",
      "# of Tweets added from this response:  477\n",
      "Total # of Tweets added:  4938\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj8dca1708weq92orfdwwdslim4d\n",
      "Start Date:  2021-07-11T01:00:00.000Z\n",
      "# of Tweets added from this response:  477\n",
      "Total # of Tweets added:  5415\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj8dks1mn2uopblgpnbl8ze12071\n",
      "Start Date:  2021-07-12T01:00:00.000Z\n",
      "# of Tweets added from this response:  499\n",
      "Total # of Tweets added:  5914\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj8dta1q4hypjp13h88rsqz67kvx\n",
      "Start Date:  2021-07-13T01:00:00.000Z\n",
      "# of Tweets added from this response:  499\n",
      "Total # of Tweets added:  6413\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj8sm2qfgxxree6vstm3n7ticsql\n",
      "Start Date:  2021-07-14T01:00:00.000Z\n",
      "# of Tweets added from this response:  500\n",
      "Total # of Tweets added:  6913\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj8ssj0cxe71il6nigv8jcyx5vnh\n",
      "Start Date:  2021-07-15T01:00:00.000Z\n",
      "# of Tweets added from this response:  497\n",
      "Total # of Tweets added:  7410\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj8t104aoygjz1xuo7lwr01gm5bx\n",
      "Start Date:  2021-07-16T01:00:00.000Z\n",
      "# of Tweets added from this response:  499\n",
      "Total # of Tweets added:  7909\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj97rrdnwxmk7dfoneku7rgmhta5\n",
      "Start Date:  2021-07-17T01:00:00.000Z\n",
      "# of Tweets added from this response:  498\n",
      "Total # of Tweets added:  8407\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj9809dicjimuejpiutvjdtlma2l\n",
      "Start Date:  2021-07-18T01:00:00.000Z\n",
      "# of Tweets added from this response:  459\n",
      "Total # of Tweets added:  8866\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj9mt1rrbt5zpzpvz4jd4r1tvse5\n",
      "Start Date:  2021-07-19T01:00:00.000Z\n",
      "# of Tweets added from this response:  493\n",
      "Total # of Tweets added:  9359\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj9mzhqxv3b4um2dbxil4xi5dbp9\n",
      "Start Date:  2021-07-20T01:00:00.000Z\n",
      "# of Tweets added from this response:  482\n",
      "Total # of Tweets added:  9841\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdj9n7zg4ez9fda41r10fbtxymnp9\n",
      "Start Date:  2021-07-21T01:00:00.000Z\n",
      "# of Tweets added from this response:  493\n",
      "Total # of Tweets added:  10334\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdja20ru5u7ch8anifyuzqe5bng59\n",
      "Start Date:  2021-07-22T01:00:00.000Z\n",
      "# of Tweets added from this response:  490\n",
      "Total # of Tweets added:  10824\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdja277i9c6h13a2l1sc8518ps58d\n",
      "Start Date:  2021-07-23T01:00:00.000Z\n",
      "# of Tweets added from this response:  494\n",
      "Total # of Tweets added:  11318\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdja2fpiixdi14io0jvlsuy1yg7st\n",
      "Start Date:  2021-07-24T01:00:00.000Z\n",
      "# of Tweets added from this response:  498\n",
      "Total # of Tweets added:  11816\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdjah6gh3oh4tmf3ysmy2fpcfxe9p\n",
      "Start Date:  2021-07-25T01:00:00.000Z\n",
      "# of Tweets added from this response:  497\n",
      "Total # of Tweets added:  12313\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdjahexl4juffjlgmug3totwv1y7x\n",
      "Start Date:  2021-07-26T01:00:00.000Z\n",
      "# of Tweets added from this response:  495\n",
      "Total # of Tweets added:  12808\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdjahng6js2dixrmc3050xjwq2ph9\n",
      "Start Date:  2021-07-27T01:00:00.000Z\n",
      "# of Tweets added from this response:  480\n",
      "Total # of Tweets added:  13288\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdm6gq40rh6jz97rozwr8lhq04kfx\n",
      "Start Date:  2021-07-28T01:00:00.000Z\n",
      "# of Tweets added from this response:  487\n",
      "Total # of Tweets added:  13775\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdm6gymbiujqkauwfv88kcf6p56rh\n",
      "Start Date:  2021-07-29T01:00:00.000Z\n",
      "# of Tweets added from this response:  485\n",
      "Total # of Tweets added:  14260\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpdm6vrf0864psc2gqvbhz8cel8tj1\n",
      "Start Date:  2021-07-30T01:00:00.000Z\n",
      "# of Tweets added from this response:  476\n",
      "Total # of Tweets added:  14736\n",
      "-------------------\n",
      "Total number of results:  14736\n"
     ]
    }
   ],
   "source": [
    "# Inputs for the request\n",
    "keywords_list = ua_common_words + ' -is:retweet place_country:UA'\n",
    "\n",
    "# Define other input parameters\n",
    "start_list  = [('2021-07-' + str(i) + 'T01:00:00.000Z') for i in range(1,31)] # Start of search period (can enter list if searching multiple distinct periods)\n",
    "end_list    = [('2021-07-' + str(i) + 'T13:00:00.000Z') for i in range(1,31)] # End   of search period (can enter list if searching multiple distinct periods)\n",
    "max_results = 500                          # Max tweets returned per call\n",
    "max_count   = 300                          # Max tweets called in total per time period\n",
    "pathway     = rp + dp + \"tweets/ukraine_tweets_01072021_01082021.csv\" # Where to save / what to call resulting csv file\n",
    "\n",
    "call_tweets(keywords_list,start_list,end_list,max_results,max_count,pathway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Russian tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the request\n",
    "keywords_list = ru_common_words + ' -is:retweet place_country:RU'\n",
    "\n",
    "call_count += 1\n",
    "\n",
    "# Define other input parameters\n",
    "start_list  = ['2021-06-30T13:00:00.000Z','2021-07-01T13:00:00.000Z','2021-07-02T13:00:00.000Z',\n",
    "               '2021-07-03T13:00:00.000Z','2021-07-04T13:00:00.000Z','2021-07-05T13:00:00.000Z',\n",
    "               '2021-07-06T13:00:00.000Z','2021-07-07T13:00:00.000Z','2021-07-08T13:00:00.000Z','2021-07-09T13:00:00.000Z'] # Start of search period (can enter list if searching multiple distinct periods)\n",
    "end_list    = ['2021-07-01T13:00:00.000Z','2021-07-02T13:00:00.000Z','2021-07-03T13:00:00.000Z',\n",
    "               '2021-07-04T13:00:00.000Z','2021-07-05T13:00:00.000Z','2021-07-06T13:00:00.000Z',\n",
    "               '2021-07-07T13:00:00.000Z','2021-07-08T13:00:00.000Z','2021-07-09T13:00:00.000Z','2021-07-10T13:00:00.000Z'] # End   of search period (can enter list if searching multiple distinct periods)\n",
    "max_results = 10                         # Max tweets returned per call\n",
    "max_count   = 3                         # Max tweets called in total per time period\n",
    "pathway     = rp + dp + \"tweets/russian_tweets_test_\" + str(call_count) + \".csv\" # Where to save / what to call resulting csv file\n",
    "\n",
    "call_tweets(keywords_list,start_list[0:1],end_list[0:1],max_results,max_count,pathway)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
