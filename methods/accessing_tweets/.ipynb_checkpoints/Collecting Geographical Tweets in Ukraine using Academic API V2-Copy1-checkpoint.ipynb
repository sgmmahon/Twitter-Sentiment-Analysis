{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "from datetime import timedelta, date\n",
    "# To add wait time between requests\n",
    "import time\n",
    "# For matching string expressions\n",
    "import re\n",
    "# For generating summary statistics\n",
    "import statistics as st\n",
    "# For loading in locally saved environmental variables\n",
    "from dotenv import load_dotenv\n",
    "# For sending SMS messages via the Twilio API\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Load in environmental variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define Twilio credentials and client\n",
    "account_sid = os.environ['TWILIO_ACCOUNT_SID']\n",
    "auth_token  = os.environ['TWILIO_AUTH_TOKEN']\n",
    "client = Client(account_sid, auth_token)\n",
    "\n",
    "# Define rootpaths\n",
    "rp  = 'C:\\\\Users\\\\sgmmahon\\\\Documents\\\\GitHub\\\\iom_project\\\\'\n",
    "rp2 = 'C:\\\\Users\\\\sgmmahon\\\\OneDrive - The University of Liverpool\\\\PhD\\\\Data\\\\Twitter Data\\\\'\n",
    "mp  = 'methods\\\\accessing_tweets\\\\'\n",
    "dp  = 'data\\\\tweet_data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions which concatenates vectors\n",
    "def cnct (x): return(\" OR \".join(x))\n",
    "def cnctwb (x): return(\"(\" + \" OR \".join(x) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query for this search is a list of the 100 most common Ukrainian words\n",
    "# https://1000mostcommonwords.com/1000-most-common-ukrainian-words/\n",
    "\n",
    "ua_common_words = [\"як\", \"Я\", \"його\", \"що\", \"він\", \"було\", \"для\", \"на\", \"є\", \"еякі\", \"вони\", \"бути\", \"у\", \"один\", \"мати\", \"це\", \n",
    "                   \"від\", \"по\", \"гаряча\", \"слово\", \"але\", \"що\", \"деякі\", \"вогонь\", \"це\", \"ви\", \"або\", \"було\", \"план\", \"и\", \"до\", \n",
    "                   \"і\", \"кішка\", \"в\", \"ми\", \"може\", \"чере\", \"другий\", \"були\", \"які\", \"зробити\", \"їх\", \"час\", \"якщо\", \"буде\", \"як\", \n",
    "                   \"аначений\", \"вона\", \"кожен\", \"скаати\", \"робить\", \"набір\", \"три\", \"хотіти\", \"повітря\", \"добре\", \"також\", \"грати\",\n",
    "                   \"невеликої\", \"кінець\", \"ставити\", \"додому\", \"читати\", \"рука\", \"порт\", \"великий\", \"аклинань\", \"додавати\", \n",
    "                   \"навіть\", \"емля\", \"тут\", \"повинні\", \"великий\", \"високий\", \"таких\", \"слідувати\", \"акт\", \"чому\", \"спитаєте\", \n",
    "                   \"чоловіки\", \"мінення\", \"пішов\", \"світло\", \"вид\", \"від\", \"потрібно\", \"будинок\", \"картинка\", \"спробуйте\", \"нам\", \n",
    "                   \"ову\", \"тварин\", \"точка\", \"мать\", \"світ\", \"рядом\", \"будувати\", \"самостійно\", \"емля\", \"батько\"]\n",
    "\n",
    "ua_common_words = cnctwb(ua_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of date objects between two specified dates\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)\n",
    "\n",
    "# Function to create list of string datetimes in a format that the Twitter API can accept\n",
    "def get_datetimes(start_dt, end_dt, time):\n",
    "    \n",
    "    # Create empty list to populate with datetime strings\n",
    "    datetimes = []\n",
    "    \n",
    "    # Append dates with the specified time attached\n",
    "    for dt in daterange(start_dt, end_dt):\n",
    "        datetimes.append(dt.strftime(\"%Y-%m-%d\") + time)\n",
    "    \n",
    "    # Return list of datetime strings\n",
    "    return(datetimes)\n",
    "\n",
    "# Define function which retrieves twitter token from the environment\n",
    "def auth():\n",
    "    return os.getenv('TWITTER_TOKEN')\n",
    "\n",
    "# Define function that uses bearer token to create headers used to access the API\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "# Function to create url to make GET request\n",
    "def create_url(query, start_time, end_time, max_results = 10, user_nm = None):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    # If not a username seach, make call using 'common_words' defined above\n",
    "    if user_nm == None:\n",
    "        # Define params you want returned\n",
    "        query_params = {# Required parameters\n",
    "            'query': query,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'max_results': max_results,\n",
    "            # Additional parameters which can be requested optionally\n",
    "            'expansions': 'author_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id,geo.place_id,entities.mentions.username',\n",
    "            'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities,possibly_sensitive',\n",
    "            'user.fields': 'id,name,username,created_at,description,public_metrics,verified,location,url',\n",
    "            'place.fields': 'full_name,id,country,country_code,contained_within,geo,name,place_type',\n",
    "            # ID to call next page of tweets\n",
    "            'next_token': {}}\n",
    "    # If a username seach, make call using the username\n",
    "    else:\n",
    "        # Define params you want returned\n",
    "        query_params = {# Required parameters\n",
    "            'query': '(from:' + query + ' -is:retweet)',\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'max_results': max_results,\n",
    "            # Additional parameters which can be requested optionally\n",
    "            'expansions': 'author_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id,geo.place_id,entities.mentions.username',\n",
    "            'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities,possibly_sensitive',\n",
    "            'user.fields': 'id,name,username,created_at,description,public_metrics,verified,location,url',\n",
    "            'place.fields': 'full_name,id,country,country_code,contained_within,geo,name,place_type',\n",
    "            # ID to call next page of tweets\n",
    "            'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "# Function to make GET request\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "# Function to obtain variable names\n",
    "def get_var_name(variable):\n",
    " for name in globals():\n",
    "     if eval(name) == variable:\n",
    "        return(name)\n",
    "\n",
    "\n",
    "# Function to append results to a csv\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    # A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    # Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    # If at least one tweet called contains geographic information\n",
    "    if 'places' in json_response['includes']:\n",
    "        # Create a dataframe of places called within this batch of tweets\n",
    "        places = pd.DataFrame(columns=['place_id','place_name','full_place_name','bbox','lat','long','place_type','country_code','country'])\n",
    "        # For loop which appends information about each place to the places dataframe \n",
    "        for place in json_response['includes']['places']:\n",
    "            # Collate all place information as dictionary\n",
    "            place_data = {'place_id'       :place['id'          ],\n",
    "                          'place_name'     :place['name'        ],\n",
    "                          'full_place_name':place['full_name'   ],\n",
    "                          'bbox'           :[' '.join(str(coord) for coord in place['geo']['bbox' ])],\n",
    "                          'lat'            :st.mean([place['geo']['bbox'][0],place['geo']['bbox'][2]]),\n",
    "                          'long'           :st.mean([place['geo']['bbox'][1],place['geo']['bbox'][3]]),\n",
    "                          'place_type'     :place['place_type'  ],\n",
    "                          'country_code'   :place['country_code'],\n",
    "                          'country'        :place['country'     ]}\n",
    "        \n",
    "            # Convert dictionary to a single-row dataframe\n",
    "            place_data = pd.DataFrame(place_data)\n",
    "            # Append place information to the places dataframe\n",
    "            places = places.append(place_data)\n",
    "    \n",
    "        # Drop duplicate places and reset index of places\n",
    "        places = places.drop_duplicates(subset=['place_id']).reset_index().drop('index',axis=1)\n",
    "        \n",
    "    #print(places)\n",
    "    \n",
    "    # Create a dataframe of users mentioned in this batch of tweets\n",
    "    users = pd.DataFrame(columns=['user_id','username','user_name','followers_count','following_count',\n",
    "                                  'tweet_count','listed_count','user_url','user_loc','user_desc'])\n",
    "    \n",
    "    # For loop which appends information about each user to the users dataframe\n",
    "    for user in json_response['includes']['users']:\n",
    "        # Collate all user information as dictionary\n",
    "        user_data = {'user_id'        :user['id'         ],\n",
    "                     'username'       :user['username'   ],\n",
    "                     'user_name'      :user['name'       ],\n",
    "                     'user_url'       :user['url'        ],\n",
    "                     'user_desc'      :user['description'],\n",
    "                     'followers_count':user['public_metrics']['followers_count'],\n",
    "                     'following_count':user['public_metrics']['following_count'],\n",
    "                     'tweet_count'    :user['public_metrics']['tweet_count'    ],\n",
    "                     'listed_count'   :user['public_metrics']['listed_count'   ]}\n",
    "    \n",
    "        if 'location' in user:\n",
    "            user_data.update({'user_loc': [user['location']]})\n",
    "        else:\n",
    "            user_data.update({'user_loc': [' ']})\n",
    "        \n",
    "        # Convert dictionary to a single-row dataframe\n",
    "        user_data = pd.DataFrame(user_data)\n",
    "        # Append place information to the places dataframe\n",
    "        users = users.append(user_data)\n",
    "    \n",
    "    # Drop duplicate entries and reset index of users\n",
    "    users = users.drop_duplicates(subset=['user_id']).reset_index().drop('index',axis=1)\n",
    "    \n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "        \n",
    "        # 4. Geolocation\n",
    "        \n",
    "        # If tweet contains geographical information\n",
    "        if ('geo' in tweet):\n",
    "            \n",
    "            # Subset places dataframe to only include information about place mentioned in tweet\n",
    "            place = places[places['place_id'] == tweet['geo']['place_id']]\n",
    "            \n",
    "            # Check whether place ID tagged in tweet is included in places listed in the response\n",
    "            # (if not, place will have no rows, so can't be subsetted)\n",
    "            try:\n",
    "                place.loc[place.index[0],:]\n",
    "            # If selecting the first row place causes an error...\n",
    "            except:\n",
    "                # Keep tweet ID and assign all other variables as empty\n",
    "                place_id        = tweet['geo']['place_id']\n",
    "                place_name      = ''\n",
    "                full_place_name = ''\n",
    "                bbox            = ''\n",
    "                place_type      = ''\n",
    "                country_code    = ''\n",
    "                country         = ''\n",
    "            # If place is includes in json response's listed places...\n",
    "            else:\n",
    "                # Assign geographical variables using place information.\n",
    "                place_id        = tweet['geo']['place_id']\n",
    "                place_name      = place.loc[place.index[0],'place_name']\n",
    "                full_place_name = place.loc[place.index[0],'full_place_name']\n",
    "                bbox            = place.loc[place.index[0],'bbox']\n",
    "                place_type      = place.loc[place.index[0],'place_type']\n",
    "                country_code    = place.loc[place.index[0],'country_code']\n",
    "                country         = place.loc[place.index[0],'country']\n",
    "            \n",
    "            \n",
    "            \n",
    "            # If tweet contains exact coordinates, provide them and assign exact_coords as True\n",
    "            if ('coordinates' in tweet['geo']):\n",
    "                lat             = tweet['geo']['coordinates']['coordinates'][0]\n",
    "                long            = tweet['geo']['coordinates']['coordinates'][1]\n",
    "                exact_coords    = True\n",
    "            # If tweet doesn't contains exact coordinates, provide centre of place bounding box and assign exact_coords as False\n",
    "            else:\n",
    "                # Check whether place ID tagged in tweet is included in places listed in the response\n",
    "                # (if not, place will have no rows, so can't be subsetted)\n",
    "                try:\n",
    "                    place.loc[place.index[0],:]\n",
    "                # If selecting the first row of place causes an error, assign all variables as empty\n",
    "                except:\n",
    "                    lat             = ''\n",
    "                    long            = ''\n",
    "                    exact_coords    = False\n",
    "                # If place is includes in json response's listed places, assign geographical variables\n",
    "                else:\n",
    "                    lat             = place.loc[place.index[0],'lat']\n",
    "                    long            = place.loc[place.index[0],'long']\n",
    "                    exact_coords    = False\n",
    "        \n",
    "        # If no geographical information provided, assign all geographical variables as blank\n",
    "        else:\n",
    "            place_id        = \" \"\n",
    "            place_name      = \" \"\n",
    "            full_place_name = \" \"\n",
    "            bbox            = \" \"\n",
    "            place_type      = \" \"\n",
    "            country_code    = \" \"\n",
    "            country         = \" \"\n",
    "            lat             = \" \"\n",
    "            long            = \" \"\n",
    "            exact_coords    = \" \"\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count   = tweet['public_metrics']['reply_count']\n",
    "        like_count    = tweet['public_metrics']['like_count']\n",
    "        quote_count   = tweet['public_metrics']['quote_count']\n",
    "        \n",
    "        # 7. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # 8. Users\n",
    "        \n",
    "        # Subset users dataframe to only include information about user mentioned who tweeted\n",
    "        user = users[users['user_id'] == tweet['author_id']]\n",
    "        \n",
    "        # Assign geographical variables unsing place information\n",
    "        username        = user.loc[user.index[0],'username']\n",
    "        user_name       = user.loc[user.index[0],'user_name']\n",
    "        followers_count = user.loc[user.index[0],'followers_count']\n",
    "        following_count = user.loc[user.index[0],'following_count']\n",
    "        tweet_count     = user.loc[user.index[0],'tweet_count']\n",
    "        listed_count    = user.loc[user.index[0],'listed_count']\n",
    "        user_url        = user.loc[user.index[0],'user_url']\n",
    "        user_loc        = user.loc[user.index[0],'user_loc']\n",
    "        user_desc       = user.loc[user.index[0],'user_desc']\n",
    "        \n",
    "        # 9. Source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 10. Conversation_id\n",
    "        conversation_id = tweet['conversation_id']\n",
    "\n",
    "        # 11. Reply settings\n",
    "        reply_settings = tweet['reply_settings']\n",
    "\n",
    "        # 12. Referenced tweets\n",
    "        if ('referenced_tweets' in tweet):   \n",
    "            referenced_tweets_type = tweet['referenced_tweets'][0]['type']\n",
    "            referenced_tweets_id   = tweet['referenced_tweets'][0]['id']\n",
    "        else:\n",
    "            referenced_tweets_type = \" \"\n",
    "            referenced_tweets_id   = \" \"\n",
    "\n",
    "        # 13. In reply to user id\n",
    "        if ('in_reply_to_user_id' in tweet):  \n",
    "            in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id = \" \"\n",
    "            \n",
    "        # 14. Entities\n",
    "        if ('entities' in tweet and 'annotations' in tweet['entities']):\n",
    "            annotations = tweet['entities']['annotations']\n",
    "        else:\n",
    "            annotations = \" \"\n",
    "        \n",
    "        # 15. Mentions\n",
    "        if ('entities' in tweet and 'mentions' in tweet['entities']):\n",
    "            mentions = tweet['entities']['mentions']\n",
    "        else:\n",
    "            mentions = \" \"\n",
    "        \n",
    "        # 16. URLs\n",
    "        if ('entities' in tweet and 'urls' in tweet['entities']):\n",
    "            linked_url = tweet['entities']['urls'][0]['expanded_url']\n",
    "        else:\n",
    "            linked_url = \" \"\n",
    "            \n",
    "        # 17. Possibly sensitive\n",
    "        if('possibly_sensitive' in tweet):\n",
    "            possibly_sensitive = tweet['possibly_sensitive']\n",
    "        else:\n",
    "            possibly_sensitive = \" \"\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [tweet_id, created_at, author_id, \n",
    "               place_id, place_name, full_place_name, lat, long, exact_coords, bbox, place_type, country_code, country,\n",
    "               lang, retweet_count, reply_count, like_count, quote_count, text, \n",
    "               username, user_name, followers_count, following_count, tweet_count, listed_count, user_url, user_loc, user_desc,\n",
    "               source, conversation_id, reply_settings, referenced_tweets_type, referenced_tweets_id, in_reply_to_user_id, \n",
    "               annotations, mentions, linked_url, possibly_sensitive]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)\n",
    "\n",
    "\n",
    "# Function that uses looping to make multiple calls, to ensure the desired number of tweets during each period have been collected \n",
    "def call_tweets(keywords,start_list,end_list,max_results,max_count,pathway,user_nm = None):\n",
    "    \n",
    "    #Inputs for tweets\n",
    "    bearer_token = auth()\n",
    "    headers = create_headers(bearer_token)\n",
    "    \n",
    "    # If csv file does not exist, create and add header row\n",
    "    if not os.path.isfile(pathway):\n",
    "        # Create csv file\n",
    "        csvFile = open(pathway, 'w', newline='', encoding='utf-8')\n",
    "        # Format for writing\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        # Write headers in first row    \n",
    "        csvWriter.writerow(['tweet_id', 'created_at', 'author_id', \n",
    "                            'place_id', 'place_name', 'full_place_name', 'lat', 'long', 'exact_coords', 'bbox', 'place_type', 'country_code', 'country', \n",
    "                            'lang', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'text', \n",
    "                            'username', 'user_name', 'followers_count', 'following_count', 'tweet_count', 'listed_count', 'user_url', 'user_loc', 'user_desc', \n",
    "                            'source', 'conversation_id', 'reply_settings', 'referenced_tweets_type', 'referenced_tweets_id', 'in_reply_to_user_id', \n",
    "                            'annotations', 'mentions', 'linked_url', 'possibly_sensitive'])\n",
    "        # Close csv file\n",
    "        csvFile.close()\n",
    "\n",
    "    #Total number of tweets we collected from the loop\n",
    "    total_tweets = 0\n",
    "    \n",
    "    # For loop which calls tweets until the max number per period (max_results) has been reached\n",
    "    for i in range(0,len(start_list)):\n",
    "        \n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            \n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            \n",
    "            # Call tweets\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(keywords, start_list[i],end_list[i], max_results, user_nm = user_nm)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            \n",
    "            # Get number of tweets called\n",
    "            result_count = json_response['meta']['result_count']\n",
    "            \n",
    "            # If 'next_token' provided\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                # Print next_token\n",
    "                print(\"Next Token: \", next_token)\n",
    "                # If results have been successfully called and additional results are ready\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    # Append results to csv and print progress\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, pathway)\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5.5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                # If results have been returned\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print('Next token:  None')\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, pathway)\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5.5)\n",
    "                # If no results returned\n",
    "                else:\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    print(\"# of Tweets added from this response: 0\")\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(1.1)\n",
    "            \n",
    "                # Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "                \n",
    "    # Print total number of tweets called            \n",
    "    print(\"Total number of results: \", total_tweets)\n",
    "\n",
    "# Function to call tweets whilst catching and treating errors\n",
    "def call_tweets_try_except(keywords,mn,mx,start_list,end_list,max_results,max_count,pathway):\n",
    "    # Create empty list to populate with users for whom data could not be retrieved\n",
    "    failed_date_requests = []\n",
    "    \n",
    "    # Set the user error counter to 0. This object records how many consequtive users tweets could not be obtained for.\n",
    "    # When it reaches 3, the function stops requesting tweets and sends and SMS reporting the errors.\n",
    "    date_err = 0\n",
    "    \n",
    "    # For loop which iterates through users\n",
    "    for dt in range(mn,mx):\n",
    "        \n",
    "        # Get the start and end time stamps to be requested in this loop iteration\n",
    "        strt = []\n",
    "        end  = []\n",
    "        strt.append(start_list[dt])\n",
    "        end.append(end_list[dt])\n",
    "        \n",
    "        # Extract date from end list\n",
    "        try:\n",
    "            int(start_list[dt][8:10])\n",
    "        except:\n",
    "            day = start_list[dt][0:9]\n",
    "        else:\n",
    "            day = start_list[dt][0:10]\n",
    "        \n",
    "        # Print the iteration number and username of the defined user\n",
    "        print(' ')\n",
    "        print('Date: ' + day)\n",
    "        \n",
    "        # Set the error counter to 0. This object records the number of times the call_tweets function has returned an \n",
    "        # exception for a given date. After 3 consecutive failures, the function will move onto the next date.\n",
    "        err = 0\n",
    "        \n",
    "        # While loop which keeps attempting to call tweets until err >= 4\n",
    "        while err < 4:\n",
    "            # Attempt to call tweets\n",
    "            try:\n",
    "                call_tweets(keywords,strt,end,max_results,max_count,pathway)\n",
    "            # If an exception occurs...\n",
    "            except Exception as e:\n",
    "                # Add 1 to the error counter\n",
    "                err += 1\n",
    "                # If less than 3 errors have occured for this date...\n",
    "                if err < 4:\n",
    "                    # Print error message\n",
    "                    print('An error ocurred. Waiting 30s then trying again...')\n",
    "                    print('Error message:: ' + str(e))\n",
    "                    # Wait for 30s before trying again\n",
    "                    time.sleep(30)\n",
    "                # If 3 or more errors have occured for this date...\n",
    "                else:\n",
    "                    # Add 1 to the user error counter\n",
    "                    date_err += 1\n",
    "                    # Add the date to the list for which data could not be obtained.\n",
    "                    failed_date_requests.append(day)\n",
    "                    # If less then 3 consecutive dates have failed to return data...\n",
    "                    if date_err < 3:\n",
    "                        # Print error message\n",
    "                        print('Persistent errors. Trying next date...')\n",
    "                        print('Error message:: ' + str(e))\n",
    "                    # If 3 or more consecutive dates have failed to return data...\n",
    "                    else:\n",
    "                        # Print error message reporting the users who have failed to return data\n",
    "                        print('Persistent errors across dates ' + str(dt - 2) + '-' + str(dt) + '. Sending error message as SMS then halting requests...')\n",
    "                        print('Error message:: ' + str(e))\n",
    "                        # Send SMS reporting the error\n",
    "                        message = client.messages \\\n",
    "                                        .create(\n",
    "                                                body='TWITTER API UPDATE. Persistent errors across dates ' + str(dt - 2) + '-' + str(dt) + '. Sending error message as SMS then halting requests. Error message:: ' + str(e),\n",
    "                                                from_='+441618505668',\n",
    "                                                to='+447989165819'\n",
    "                                                )\n",
    "                        # Stop function\n",
    "                        return\n",
    "            # If tweets successfully called...\n",
    "            else:\n",
    "                # Reset user error counter to 0\n",
    "                date_err = 0\n",
    "                # Break out of while loop and move onto the next date\n",
    "                break\n",
    "                \n",
    "    # Once all tweets have been successfully called, report the dates for which data could not be collected.\n",
    "    print('')\n",
    "    print('Finished calling tweets. Requests failed for the following dates: ' + str(failed_date_requests)[1:(len(str(failed_date_requests))-1)])\n",
    "    \n",
    "    # Send SMS with the same information\n",
    "    message = client.messages \\\n",
    "                    .create(\n",
    "                            body='TWITTER API UPDATE. Finished calling tweets. Requests failed for the following dates: ' + str(failed_date_requests)[1:(len(str(failed_date_requests))-1)],\n",
    "                            from_='+441618505668',\n",
    "                            to='+447989165819'\n",
    "                            )    \n",
    "\n",
    "# Function to call users' tweet whilst catching and treating errors\n",
    "def call_users_try_except(usrs,mn,mx,start_list,end_list,max_results,max_count,pathway):\n",
    "    # Create empty list to populate with users for whom data could not be retrieved\n",
    "    failed_user_requests = []\n",
    "    \n",
    "    # Set the user error counter to 0. This object records how many consequtive users tweets could not be obtained for.\n",
    "    # When it reaches 3, the function stops requesting tweets and sends and SMS reporting the errors.\n",
    "    usr_err = 0\n",
    "    \n",
    "    # For loop which iterates through users\n",
    "    for usr in range(mn,mx):\n",
    "        \n",
    "        # Extract the username for the user to be requested in this loop iteration\n",
    "        user = usrs[usr]\n",
    "        \n",
    "        # Print the iteration number and username of the defined user\n",
    "        print(' ')\n",
    "        print('User ' + str(usr) + ': ' + user)\n",
    "        \n",
    "        # Set the error counter to 0. This object records the number of times the call_tweets function has returned an \n",
    "        # exception for a given user. After 3 consecutive failures, the function will move onto the next user.\n",
    "        err = 0\n",
    "        \n",
    "        # While loop which keeps attempting to call tweets until err >= 4\n",
    "        while err < 4:\n",
    "            # Attempt to call tweets\n",
    "            try:\n",
    "                call_tweets(user,start_list,end_list,max_results,max_count,pathway,user_nm = True)\n",
    "            # If an exception occurs...\n",
    "            except Exception as e:\n",
    "                # Add 1 to the error counter\n",
    "                err += 1\n",
    "                # If less than 3 errors have occured for this user...\n",
    "                if err < 4:\n",
    "                    # Print error message\n",
    "                    print('An error ocurred. Waiting 30s then trying again...')\n",
    "                    print('Error message:: ' + str(e))\n",
    "                    # Wait for 30s before trying again\n",
    "                    time.sleep(30)\n",
    "                # If 3 or more errors have occured for this user...\n",
    "                else:\n",
    "                    # Add 1 to the user error counter\n",
    "                    usr_err += 1\n",
    "                    # Add the user's username to the list of users for whome data could not be obtained.\n",
    "                    failed_user_requests.append(user)\n",
    "                    # If less then 3 consecutive users have failed to return data...\n",
    "                    if usr_err < 3:\n",
    "                        # Print error message\n",
    "                        print('Persistent errors. Trying next user...')\n",
    "                        print('Error message:: ' + str(e))\n",
    "                    # If 3 or more consecutive users have failed to return data...\n",
    "                    else:\n",
    "                        # Print error message reporting the users who have failed to return data\n",
    "                        print('Persistent errors across users ' + str(usr - 2) + '-' + str(usr) + '. Sending error message as SMS then halting requests...')\n",
    "                        print('Error message:: ' + str(e))\n",
    "                        # Send SMS reporting the error\n",
    "                        message = client.messages \\\n",
    "                                        .create(\n",
    "                                                body='TWITTER API UPDATE. Persistent errors across users ' + str(usr - 2) + '-' + str(usr) + '. Sending error message as SMS then halting requests. Error message:: ' + str(e),\n",
    "                                                from_='+441618505668',\n",
    "                                                to='+447989165819'\n",
    "                                                )\n",
    "                        # Stop function\n",
    "                        return\n",
    "            # If tweets successfully called...\n",
    "            else:\n",
    "                # Reset user error counter to 0\n",
    "                usr_err = 0\n",
    "                # Break out of while loop and move onto the next user\n",
    "                break\n",
    "                \n",
    "    # Once all tweets have been successfully called, report the users for whome data could not be collected.\n",
    "    print('Finished calling tweets. Requests failed for the following users: ' + str(failed_user_requests)[1:(len(str(failed_user_requests))-1)])\n",
    "    \n",
    "    # Send SMS with the same information\n",
    "    message = client.messages \\\n",
    "                    .create(\n",
    "                            body='TWITTER API UPDATE. Finished calling tweets. Requests failed for the following users: ' + str(failed_user_requests)[1:(len(str(failed_user_requests))-1)],\n",
    "                            from_='+441618505668',\n",
    "                            to='+447989165819'\n",
    "                            )\n",
    "\n",
    "# Function to import and format csv files\n",
    "def import_csv(x):\n",
    "    # Read in tweets as dataframe\n",
    "    x = pd.read_csv(x, low_memory = False)\n",
    "    # Drop index column if added\n",
    "    if 'Unnamed: 0' in x.columns:\n",
    "        x = x.drop(['Unnamed: 0'], axis=1)\n",
    "    # Convert date variable from str to datetime if included\n",
    "    if 'created_at' in x.columns:\n",
    "        x['created_at'] = pd.to_datetime(x['created_at'])\n",
    "    # Return df\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs for the request\n",
    "#keywords_list = ua_common_words + ' -is:retweet place_country:UA'\n",
    "#\n",
    "## Define other input parameters\n",
    "#start_list  = get_datetimes(date(2021, 6, 1), date(2022, 3, 20), 'T00:00:00.00+02:00') # Start of search period (can enter list if searching multiple distinct periods)\n",
    "#end_list    = get_datetimes(date(2021, 6, 1), date(2022, 3, 20), 'T23:59:59.99+02:00') # End   of search period (can enter list if searching multiple distinct periods)\n",
    "#max_results = 500                          # Max tweets returned per call\n",
    "#max_count   = 5000                         # Max tweets called in total per time period\n",
    "#pathway     = rp2 + 'ukraine_tweets_01062021_20032022.csv' # Where to save / what to call resulting csv file\n",
    "#\n",
    "#call_tweets_try_except(keywords_list,0,len(start_list),start_list,end_list,max_results,max_count,pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove duplicate tweets that were created calling certain days multiple times\n",
    "#\n",
    "## Define pathway\n",
    "#pathway     = rp2 + 'ukraine_tweets_01062021_20032022.csv' # Where to save / what to call resulting csv file\n",
    "#\n",
    "## Import tweets, remove duplicates, sort by datetime (earliest first) and reset index\n",
    "#tweet_sample = import_csv(pathway).drop_duplicates() \\\n",
    "                                  #.sort_values(by='created_at').reset_index().drop(['index'], axis=1)\n",
    "#\n",
    "## Save adjusted results\n",
    "#tweet_sample.to_csv(pathway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Tweets for Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tweets from previous call\n",
    "tweet_sample = import_csv(rp2 + 'ukraine_tweets_01062021_20032022.csv')\n",
    "\n",
    "# Extract list of unique users\n",
    "#usrs = list(set(tweet_sample[['author_id']].apply(lambda x: '%.5f' % x, axis = 1).apply(lambda x: str(int(float(x))))))\n",
    "usrs = list(set(tweet_sample.loc[:,'username'].tolist()))\n",
    "\n",
    "# Add geographical filter if needed\n",
    "usrs_has_geo = list(map(lambda x: x + ' has:geo', usrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other input parameters\n",
    "start_list  = ['2021-06-01T00:00:00Z','2021-11-01T00:00:00Z'] # Start of search period (can enter list if searching multiple distinct periods)\n",
    "end_list    = ['2021-10-31T23:59:59Z','2022-03-20T23:59:59Z'] # End   of search period (can enter list if searching multiple distinct periods)\n",
    "range_min   = 0                            # What point in the username list to start\n",
    "range_max   = len(usrs_has_geo)            # What point in the username list to end\n",
    "max_results = 500                          # Max tweets returned per call\n",
    "max_count   = 10000                        # Max tweets called in total per time period\n",
    "pathway     = rp + dp + \"tweets/user_name_tweets.csv\" # Where to save / what to call resulting csv file\n",
    "\n",
    "call_users_try_except(usrs,range_min,range_max,start_list,end_list,max_results,max_count,pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n"
     ]
    }
   ],
   "source": [
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "next_token = None\n",
    "\n",
    "url = create_url(usrs[57], '2021-07-01T01:00:00.000Z', '2021-07-31T13:00:00.000Z', 500, user_nm = True)\n",
    "\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1], next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follows users were bad: 'CalabriaTOP', 'duram_p', 'po_z_nyakov'\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "test = []\n",
    "for i in itertools.chain(range(5, 6), [20, 30]):\n",
    "    test.append(usrs[i])\n",
    "print('The follows users were bad: ' + str(test)[1:(len(str(test))-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follows users were bad: 1, 2, 3, 4, 5, 6\n"
     ]
    }
   ],
   "source": [
    "test = [1,2,3,4,5,6]\n",
    "print('The follows users were bad: ' + str(test)[1:(len(str(test))-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other input parameters\n",
    "start_list  = ['2021-07-01T01:00:00.000Z','2022-02-01T01:00:00.000Z'] # Start of search period (can enter list if searching multiple distinct periods)\n",
    "end_list    = ['2021-07-31T13:00:00.000Z','2022-03-09T13:00:00.000Z'] # End   of search period (can enter list if searching multiple distinct periods)\n",
    "max_results = 500                          # Max tweets returned per call\n",
    "max_count   = 400                          # Max tweets called in total per time period\n",
    "pathway     = rp + dp + \"tweets/user_name_tweets.csv\" # Where to save / what to call resulting csv file\n",
    "\n",
    "for usr in range(0,len(usrnms)):\n",
    "    print('User: ' + str(usr))\n",
    "    user = usrs[usr]\n",
    "    call_tweets(user,start_list,end_list,max_results,max_count,pathway,user_nm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key elements of twitter V2 API call \n",
    "\n",
    "json_response['data']               # Info about tweets\n",
    "\n",
    "json_response['includes']['users']  # Info about users mentioned in tweets\n",
    "\n",
    "json_response['includes']['places'] # Info about places attached to tweets\n",
    "\n",
    "json_response['includes']['tweets'] # Info about tweets that interact with primary tweets (e.g. replies, quotes)\n",
    "\n",
    "json_response['errors']             # Errors that occured when calling tweets\n",
    "\n",
    "json_response['meta']               # Meta data like newest and oldest tweets, total tweets called and next token if paginating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Russian Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.openrussian.org/list/all\n",
    "\n",
    "ru_common_words = ['и', 'в', 'не', 'на', 'что', 'тот', 'быт', 'с', 'а', 'весь', 'как', 'по', 'но', 'э́то', 'к', 'у', 'из', \n",
    "                'за', 'так', 'же', 'сказа́ть', 'э́тот', 'кото́рый', 'мочь', 'о', 'челове́к', 'ещё', 'бы', 'тако́й', 'то́лько', \n",
    "                'себя́', 'како́й', 'для', 'уже́', 'когда́', 'кто', 'вот', 'да', 'год', 'знать', 'е́сли', 'до', 'говори́ть', 'и́ли', \n",
    "                'мой', 'вре́мя', 'рука́', 'са́мый', 'нет', 'ни', 'стать', 'большо́й', 'друго́й', 'свой', 'де́ло', 'под', 'где', \n",
    "                'что́бы', 'ну', 'сам', 'есть', 'раз', 'чём', 'там', 'глаз', 'пе́рвый', 'день', 'жизнь', 'тут', 'ничто́', \n",
    "                'пото́м', 'о́чень', 'ли', 'при', 'хоте́ть', 'на́до', 'голова́', 'без', 'ви́деть', 'тепе́рь', 'идти́', 'друг', 'сейча́с', \n",
    "                'стоя́ть', 'дом', 'то́же', 'по́сле', 'мо́жно', 'сло́во', 'че́рез', 'ме́сто', 'ду́мать', 'здесь', 'спроси́ть', 'лицо́', \n",
    "                'тогда́', 'до́лжный', 'ведь', 'но́вый', 'ка́ждый']\n",
    "\n",
    "ru_common_words = cnctwb(ru_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Russian Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the request\n",
    "keywords_list = ru_common_words + ' -is:retweet place_country:RU'\n",
    "\n",
    "call_count += 1\n",
    "\n",
    "# Define other input parameters\n",
    "start_list  = ['2021-06-30T13:00:00.000Z','2021-07-01T13:00:00.000Z','2021-07-02T13:00:00.000Z',\n",
    "               '2021-07-03T13:00:00.000Z','2021-07-04T13:00:00.000Z','2021-07-05T13:00:00.000Z',\n",
    "               '2021-07-06T13:00:00.000Z','2021-07-07T13:00:00.000Z','2021-07-08T13:00:00.000Z','2021-07-09T13:00:00.000Z'] # Start of search period (can enter list if searching multiple distinct periods)\n",
    "end_list    = ['2021-07-01T13:00:00.000Z','2021-07-02T13:00:00.000Z','2021-07-03T13:00:00.000Z',\n",
    "               '2021-07-04T13:00:00.000Z','2021-07-05T13:00:00.000Z','2021-07-06T13:00:00.000Z',\n",
    "               '2021-07-07T13:00:00.000Z','2021-07-08T13:00:00.000Z','2021-07-09T13:00:00.000Z','2021-07-10T13:00:00.000Z'] # End   of search period (can enter list if searching multiple distinct periods)\n",
    "max_results = 10                         # Max tweets returned per call\n",
    "max_count   = 3                         # Max tweets called in total per time period\n",
    "pathway     = rp + dp + \"tweets/russian_tweets_test_\" + str(call_count) + \".csv\" # Where to save / what to call resulting csv file\n",
    "\n",
    "call_tweets(keywords_list,start_list[0:1],end_list[0:1],max_results,max_count,pathway)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
