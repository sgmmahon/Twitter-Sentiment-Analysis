{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "\n",
    "# Load packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# Load data\n",
    "tweets = pd.read_csv('../Data/twitter_covid_feb_small.csv', encoding='latin-1')\n",
    "\n",
    "\n",
    "### Clean tweets ###\n",
    "\n",
    "\n",
    "## Convert full text to string ##\n",
    "\n",
    "\n",
    "# Some tweets are classified by Python as 'floats' so need to convert to string for cleaning process\n",
    "tweets['full_text_str'] = tweets['full_text'].astype(str)\n",
    "\n",
    "\n",
    "## Remove punctuation ##\n",
    "\n",
    "\n",
    "# We are not interested in punctuation for analyses so replace them with a space\n",
    "# tweets['full_text_letters'] = tweets['full_text'].apply(lambda x : re.sub(r'[^a-zA-Z0-9 ]',' ',str(x)))\n",
    "# If want to get rid of numbers too use re.sub(r'[^a-zA-Z ]' - normally we would get rid of numbers as well but if we want to look at '5G' then need to leave them in\n",
    "\n",
    "    \n",
    "## Convert all words to lower case ##\n",
    "\n",
    "\n",
    "# To normalise comparisons else Love and love are treated seperately (for upper case swicth to 'word.upper())\n",
    "tweets['full_text_lower'] = tweets['full_text_str'].apply(  lambda x: ' '.join( [ word.lower() for word in x.split() ] ) )\n",
    "del tweets['full_text_str']\n",
    "\n",
    "\n",
    "## Remove stop words ##\n",
    "\n",
    "\n",
    "# Remove common words such as 'a', 'the', 'on' that do not contribute to the meaning of texts through providing unncessary information\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\") # Define stopwords\n",
    "tweets['full_text_stop'] = tweets['full_text_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) # Remove from tweet\n",
    "del tweets['full_text_lower']\n",
    "\n",
    "\n",
    "## Replace abbreviations ##\n",
    "\n",
    "\n",
    "# Convert terms such as OMG to Oh My God - I have also included RT as retweet or HT as hattip\n",
    "# Code From: https://medium.com/nerd-stuff/python-script-to-turn-text-message-abbreviations-into-actual-phrases-d5db6f489222\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"./slang.txt\"\n",
    "\n",
    "        # File Access mode [Read Mode]\n",
    "        with open(fileName, \"r\") as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9]+', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    return ' '.join(user_string)\n",
    "\n",
    "tweets['full_text_abbr'] = tweets['full_text_stop'].apply(lambda x:  translator(x)  ) \n",
    "del tweets['full_text_stop']\n",
    "\n",
    "\n",
    "## Normalising language ##\n",
    "\n",
    "\n",
    "# Terms may be used with different tenses which ae currently being treated seperately. We have two options:\n",
    "\n",
    "# 1. Stemming\n",
    "# Normalise language by converting terms to their base/root (i.e. removes -ing, -ed, -s etc) e.g. waiting becomes wait\n",
    "# Pros: computationally efficient\n",
    "# Cons: root terms may be less obvious e.g. loving becomes lov \n",
    "\n",
    "#ps = PorterStemmer()\n",
    "#tweets['Step3_SentimentText'] = tweets['full_text_stop'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "\n",
    "# 2. Lemmatization\n",
    "# Convert terms to their root dictionary form (or lemma) e.g. runs, running and ran are each forms of run\n",
    "# Pros: greater context to root terms as uses valid words\n",
    "# Cons: requires greater memory to run, does not always get to root word\n",
    "\n",
    "# We will go with Lemmatization as more useful in interpretation of words\n",
    "\n",
    "# nltk.download() # To install WordNet corpora\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tweets['full_text_cleaned'] = tweets['full_text_abbr'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "del tweets['full_text_abbr']\n",
    "\n",
    "\n",
    "## Parts of speech tagging ##\n",
    "\n",
    "\n",
    "# Define structure of terms as nouns, pronouns, verbs etc\n",
    "#tweets['full_text_pos'] = tweets['full_text_cleaned'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "\n",
    "\n",
    "## Tokenise data ##\n",
    "\n",
    "\n",
    "## Code breaks up tweets into seperate words which is neccessary to analyse and identify terms\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#tokens = tweets.full_text_cleaned.apply(word_tokenize)\n",
    "\n",
    "\n",
    "## Save ##\n",
    "\n",
    "# Save as csv file\n",
    "tweets.to_csv('../Data/twitter_covid_feb_small_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "##### Identify bots #####\n",
    "#########################\n",
    "\n",
    "# Purpose: To classify Twitter users as bots or not.\n",
    "\n",
    "# Libraries\n",
    "# devtools::install_github(\"mkearney/botrnot\") # This didn't work for me so needed to follow https://github.com/mkearney/tweetbotornot/issues/24\n",
    "# remove.packages(\"tweetbotornot\")\n",
    "# install_github(\"markagreen/tweetbotornot\", dependencies = TRUE) # Anyone should be able to install this\n",
    "library(tweetbotornot)\n",
    "#devtools::install_github(\"mkearney/rtweet\")\n",
    "library(rtweet)\n",
    "library(dplyr)\n",
    "library(httr)\n",
    "\n",
    "# Set up Twitter details\n",
    "consumer_key <- \"VzihIPxv5oFrd3SkNuBuQk9o3\"\n",
    "consumer_secret <- \"Iq7hi4K1cZnzgD3RC1miTM6rcrHMA4aeHj3OeCsI9OvFVtX5Ej\"\n",
    "access_token <- \"2507558052-K6abCbi1LD59qPxIMtlsGVWL7dDdLkfyQYPuLFS\"\n",
    "access_secret <- \"x9AwwJuLJWWdpSZMPkKGG4RAtxtZh9uXxWfebj5HsseWA\"\n",
    "app <- \"Liv_misinformation_study\"\n",
    "\n",
    "token <- create_token(\n",
    "  app = app,\n",
    "  consumer_key = consumer_key,\n",
    "  consumer_secret = consumer_secret,\n",
    "  access_token = access_token,\n",
    "  access_secret = access_secret,\n",
    "  set_renv = TRUE)\n",
    "\n",
    "rm(consumer_key, consumer_secret, access_secret, access_token, app)\n",
    "\n",
    "# Create list of usernames in project\n",
    "\n",
    "# January\n",
    "tweets <- read.csv(\"../Data/twitter_covid_jan_small_cleaned.csv\", header = TRUE) # Load data\n",
    "users <- as.data.frame(tweets$username) # Subset usernames\n",
    "users <- distinct(users) # Remove duplicates (i.e. multiple tweets within a month)\n",
    "rm(tweets) # Delete\n",
    "\n",
    "# February\n",
    "tweets <- read.csv(\"../Data/twitter_covid_feb_small_cleaned.csv\", header = TRUE) # Load data\n",
    "temp <- as.data.frame(tweets$username) # Subset usernames\n",
    "temp <- distinct(temp) # Remove duplicates\n",
    "users <- rbind(users, temp) # Join on to longer list\n",
    "users <- distinct(users) # Remove duplicates again\n",
    "rm(tweets, temp)\n",
    "\n",
    "# March\n",
    "tweets <- read.csv(\"../Data/twitter_covid_mar_small_cleaned.csv\", header = TRUE) # Load data\n",
    "temp <- as.data.frame(tweets$username) # Subset usernames\n",
    "temp <- distinct(temp) # Remove duplicates\n",
    "users <- rbind(users, temp) # Join on to longer list\n",
    "users <- distinct(users) # Remove duplicates again\n",
    "rm(tweets, temp) \n",
    "\n",
    "# Save\n",
    "write.csv(users, \"./users.csv\")\n",
    "\n",
    "# Estimate likelihood of being a bot\n",
    "\n",
    "# Running code after running above\n",
    "# user_list <- as.character(users$`tweets$username`)\n",
    "# predict_bots <- tweetbotornot(user_list, fast = TRUE)\n",
    "\n",
    "# Loading in csv file\n",
    "users <- read.csv(\"./users.csv\")\n",
    "user_list <- as.character(users$x)\n",
    "# Split data into 90k subsets as can only run that at a time (Twitter limit)\n",
    "user_list1 <- user_list[1:90000]\n",
    "user_list2 <- user_list[90001:180000]\n",
    "user_list3 <- user_list[180001:270000]\n",
    "user_list4 <- user_list[270001:360000]\n",
    "user_list5 <- user_list[360001:450000]\n",
    "user_list6 <- user_list[450001:508034]\n",
    "\n",
    "# Predict bot likelihood\n",
    "predict_bots1 <- tweetbotornot(user_list1, fast = TRUE)\n",
    "predict_bots2 <- tweetbotornot(user_list2, fast = TRUE)\n",
    "predict_bots3 <- tweetbotornot(user_list3, fast = TRUE)\n",
    "predict_bots4 <- tweetbotornot(user_list4, fast = TRUE)\n",
    "predict_bots5 <- tweetbotornot(user_list5, fast = TRUE)\n",
    "predict_bots6 <- tweetbotornot(user_list6, fast = TRUE)\n",
    "\n",
    "# Join back together\n",
    "predict_bots <- rbind(predict_bots1, predict_bots2, predict_bots3, predict_bots4, predict_bots5, predict_bots6)\n",
    "write.csv(predict_bots, \"./predict_bots.csv\") # Save\n",
    "rm(list = ls()) # Remove all files\n",
    "gc()\n",
    "\n",
    "# Note:\n",
    "# The default [gradient boosted] model uses both users-level (bio, location, number of followers and friends, etc.) and tweets-level (number of hashtags, mentions, capital letters, etc. in a user’s most recent 100 tweets) data to estimate the probability that users are bots. For larger data sets, this method can be quite slow. Due to Twitter’s REST API rate limits, users are limited to only 180 estimates per every 15 minutes.\n",
    "# To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n",
    "# I will use the fast method first (180 users per 15 mins means it would take ~30 days), and then update with the slower method when can run it longer\n",
    "\n",
    "# Test code\n",
    "users <- c(\"realdonaldtrump\", \"netflix_bot\",\n",
    "           \"kearneymw\", \"dataandme\", \"hadleywickham\",\n",
    "           \"ma_salmon\", \"juliasilge\", \"tidyversetweets\", \n",
    "           \"American__Voter\", \"mothgenerator\", \"hrbrmstr\")\n",
    "\n",
    "## get botornot estimates\n",
    "bot_list <- tweetbotornot(users, fast = FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun  8 15:35:13 2020\n",
    "\n",
    "@author: markagreen\n",
    "\"\"\"\n",
    "\n",
    "## To load the full JSONL file\n",
    "##\n",
    "#import json_lines\n",
    "#\n",
    "#def load_jsonl(file):\n",
    "#    tweets = []\n",
    "#    f=2\n",
    "#    with open(file, 'rb') as f:\n",
    "#        for tweet in json_lines.reader(f, broken=True):\n",
    "#            tweets.append(tweet)\n",
    "#    return (tweets)\n",
    "#\n",
    "#tweets = load_jsonl('../January/twitter_covid_jan_covidiots.jsonl') # Select file\n",
    "#print(tweets[0]) # Print first tweet\n",
    "\n",
    "# Load in data efficiently\n",
    "\n",
    "import json_lines\n",
    "import pprint\n",
    "\n",
    "# To visualise JSONL structure better\n",
    "def prettyprint(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            prettyprint(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "# Loads in tweets one by one\n",
    "def load_jsonl(file):\n",
    "    tweets = [] # Create balnk file to read tweets into \n",
    "    with open(file, 'rb') as f:\n",
    "        for tweet in json_lines.reader(f, broken=True): # For each tweet\n",
    "\n",
    "            reduced_tweet = { # Store key details\n",
    "                'created_at' : tweet['created_at'], # Time and date of tweet\n",
    "                'id' : tweet['id_str'], # Unique ID of Tweet\n",
    "                'username' : tweet['user']['screen_name'], # Username of Twitter profile\n",
    "                'user_id' : tweet['user']['id_str'], # Unique ID for Twtter profile\n",
    "                'text': tweet['text'] # Store text of tweet (140 characters max)\n",
    "            }\n",
    "            \n",
    "            if 'extended_tweet' in tweet: # If tweet is more than 140 characters (Twitter seperates out old and current tweet lengths)\n",
    "                reduced_tweet.update({'full_text':tweet['extended_tweet']['full_text']}) # Store full text (else cut off)\n",
    "            elif 'retweeted_status' in tweet and 'extended_tweet' in tweet['retweeted_status']: # If a retweet and tweet more than 140 characters\n",
    "                reduced_tweet.update({'full_text':tweet['retweeted_status']['extended_tweet']['full_text']}) # Store full text\n",
    "            else: # Else if neither of previous two options, keep 140 characters text\n",
    "                reduced_tweet.update({'full_text':tweet['text']})\n",
    "            \n",
    "            if 'derived' in tweet['user']: # If present in the users information\n",
    "                if 'locations' in tweet['user']['derived']: # Store country\n",
    "                    reduced_tweet.update({'country':tweet['user']['derived']['locations'][0]['country']})\n",
    "#                else:\n",
    "#                    reduced_tweet.update({}'country':''}) # If not present then store as missing\n",
    "                \n",
    "                if 'region' in tweet['user']['derived']['locations'][0]: # If present in the users information\n",
    "                    reduced_tweet.update({'region':tweet['user']['derived']['locations'][0]['region']}) # Store region\n",
    "#                else:\n",
    "#                    reduced_tweet.update({'region':''}) # If not present then store as missing\n",
    "            \n",
    "            if 'retweeted_status' in tweet: # If a retweet (store as nested within same Tweet)\n",
    "               reduced_tweet.update({'retweeted_user':{\n",
    "                                       'user_id' : tweet['retweeted_status']['user']['id_str'], # Store user ID of retweeted user\n",
    "                                       'username' : tweet['retweeted_status']['user']['screen_name']}}) # Store username\n",
    "                \n",
    "            #print(\"######################### \") # Prints progress (used for testing purposes to check code)\n",
    "            #prettyprint(reduced_tweet)\n",
    "                                \n",
    "            tweets.append(reduced_tweet)\n",
    "    return (tweets)\n",
    "\n",
    "tweets = load_jsonl('../Data/twitter_covid_feb.jsonl') # Load specific file\n",
    "print(tweets[10]) # Check loaded in fine\n",
    "\n",
    "# Save\n",
    "import json\n",
    "with open('../Data/twitter_covid_feb_small.jsonl', 'w') as outfile:\n",
    "    json.dump(tweets, outfile)\n",
    "\n",
    "# Convert file to data frame\n",
    "import pandas as pd\n",
    "with open('../Data/twitter_covid_feb_small.jsonl', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data) # Converts to data frame\n",
    "pd.set_option('display.max_columns', None) # So can view all columns\n",
    "df.head(1) # Check has worked\n",
    "\n",
    "# Save as csv file\n",
    "df.to_csv('../Data/twitter_covid_feb_small.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun  8 13:59:44 2020\n",
    "\n",
    "@author: Mark Green\n",
    "\"\"\"\n",
    "\n",
    "# Define details of our account \n",
    "API_KEY = 'VzihIPxv5oFrd3SkNuBuQk9o3'\n",
    "API_SECRET_KEY = 'Iq7hi4K1cZnzgD3RC1miTM6rcrHMA4aeHj3OeCsI9OvFVtX5Ej'\n",
    "DEV_ENVIRONMENT_LABEL = 'datacollection'\n",
    "API_SCOPE = 'fullarchive'  # 'fullarchive' for full archive, '30day' for last 31 days\n",
    "\n",
    "# Define search terms \n",
    "SEARCH_QUERY = '(covid-19 OR Corona OR Coronavirus OR \"The virus\" OR Covid OR Covid19 OR Rona OR C19 OR COV-19 OR Corona-19 OR CV19 OR CV-19 OR CV OR NCov OR 2019nCov OR SARS-CoV-2 OR Coronaoutbreak OR Coronaapocalypse OR Carona OR Codvid OR Coronavid OR Corono OR Coron OR Covit OR Curona OR Corrona OR Covd OR Korona OR koronavirus OR Corvid OR corvid-19 OR covid_19uk OR covid-19uk OR Briefing_COVID19 OR coronavirusuk OR COVID_19uk OR covid19uk OR CoronavirusBillUK OR UKCoronavirusBill OR uklockdown OR \"Chinese virus\" OR chinesevirus OR \"Wuhan virus\" OR wuhanvirus OR Boomerremover OR Covididiot OR Covididiots OR covidiot OR covidiots OR Kung Flu) lang:en (place_country:GB OR profile_country:GB)' # Max 1024 characters\n",
    "\n",
    "# SEARCH_QUERY = 'from:snopes' # For specific Twitter users\n",
    "\n",
    "RESULTS_PER_CALL = 500  # 100 for sandbox, 500 for paid tiers\n",
    "FROM_DATE = '2020-03-02 00:00'  # format YYYY-MM-DD HH:MM (hour and minutes optional)\n",
    "TO_DATE = '2020-03-10 00:00' # format YYYY-MM-DD HH:MM (hour and minutes optional)\n",
    "# E.g. TO_DATE = 2020-02-14 00:00 starts at Feb 13 23:59:59 +0000 2020 back\n",
    "# FROM_DATE = 2020-02-08 00:00 ends at Feb 08 00:00:00 +0000 2020\n",
    "MAX_RESULTS = 100000000  # Number of Tweets you want to collect e.g. 1000000\n",
    "\n",
    "# Last downloaded 1st March (~81k)\n",
    "\n",
    "# Search 1: Covid-related terms\n",
    "\n",
    "# General terms\n",
    "# covid-19 OR Corona OR Coronavirus OR The virus OR Covid OR Covid19 OR Rona OR C19 OR COV-19 OR Corona-19 OR CV19 OR \n",
    "# CV-19 OR CV OR NCov OR 2019nCov OR SARS-CoV-2 OR Coronaoutbreak OR Coronaapocalypse\n",
    "\n",
    "# Misspellings \n",
    "# Carona OR Codvid OR Coronavid OR Corono OR Coron OR Covit OR Curona OR Corrona OR Covd OR Korona OR koronavirus OR\n",
    "# Corvid OR corvid-19 OR\n",
    "\n",
    "# UK specific\n",
    "# covid_19uk OR covid-19uk OR Briefing_COVID19 OR coronavirusuk OR COVID_19uk OR covid19uk OR CoronavirusBillUK\n",
    "# UKCoronavirusBill OR uklockdown\n",
    "\n",
    "# Derogatory terms or discrimination \n",
    "# Chinese virus OR chinesevirus OR Wuhan virus OR wuhanvirus OR Boomerremover OR Covididiot OR Kung Flu\n",
    "\n",
    "# Search 2: Public Health / policy / treatment terms \n",
    "\n",
    "# Testing OR trace OR PPE OR ppeshortage OR Antibody OR antibodies OR Stay alert OR control the virus OR save lives OR stayalert OR\n",
    "# Stay at home OR stayathome OR stayhome OR Lockdown OR uklockdown OR Quarantine OR quarentine OR quarantaine OR Isolation OR iso OR\n",
    "# Self isolate OR Wash your hands OR washyourhands OR washurhands OR Social distancing OR socialdistancing OR social distance OR\n",
    "# SocialDistancingNow OR Shielding OR sheltering OR Lockdown OR lockdown fatigue OR Mask OR n95 OR hydroxychloroquine OR\n",
    "# clapforcarers\n",
    "\n",
    "# Specific searches\n",
    "# lang:en # Only English tweets\n",
    "# (place_country:GB OR profile_country:GB) # Only UK tweets\n",
    "\n",
    "# Define where Tweets should be saved\n",
    "FILENAME = '../Data/twitter_covid_mar.jsonl'\n",
    "\n",
    "# Print update for every X tweets downloaded\n",
    "PRINT_AFTER_X = 500\n",
    "\n",
    "# Define YAML with key details for accessing Twitter API\n",
    "import yaml\n",
    "config = dict(\n",
    "    search_tweets_api=dict(\n",
    "        account_type='premium',\n",
    "        endpoint=f\"https://api.twitter.com/1.1/tweets/search/fullarchive/datacollection.json\",\n",
    "        consumer_key=API_KEY,\n",
    "        consumer_secret=API_SECRET_KEY\n",
    "    )\n",
    ")\n",
    "\n",
    "with open('twitter_keys.yaml', 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)\n",
    "\n",
    "## We can test searches in the sandbox environment without adding to monthly count here\n",
    "# from searchtweets import collect_results, gen_rule_payload\n",
    "# rule = gen_rule_payload(\"beyonce\", results_per_call=100) # define search terms\n",
    "# tweets = collect_results(rule, max_results=100) # Collect tweets\n",
    "# [print(tweet.all_text, end='\\n\\n') for tweet in tweets[0:10]]; # Print\n",
    "    \n",
    "import json\n",
    "from searchtweets import load_credentials, gen_rule_payload, ResultStream\n",
    "\n",
    "# Define rules for premium search for streaming tweets\n",
    "premium_search_args = load_credentials(\"twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "\n",
    "# Put together search terms and rules from earlier\n",
    "rule = gen_rule_payload(SEARCH_QUERY,\n",
    "                        results_per_call=RESULTS_PER_CALL,\n",
    "                        from_date=FROM_DATE,\n",
    "                        to_date=TO_DATE\n",
    "                        )\n",
    "\n",
    "# Stream tweets rather than download in one go\n",
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=MAX_RESULTS,\n",
    "                  **premium_search_args)\n",
    "\n",
    "# Access API and save each tweet as single line on JSON lines file\n",
    "with open(FILENAME, 'a', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for tweet in rs.stream():\n",
    "        n += 1\n",
    "        if n % PRINT_AFTER_X == 0:\n",
    "            print('{0}: {1}'.format(str(n), tweet['created_at']))\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Misinformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classifying tweets as misinformation ###\n",
    "\n",
    "# Libraries\n",
    "\n",
    "# Load data\n",
    "jan <- read.csv(\"./Data/twitter_covid_jan_small_cleaned.csv\", header = TRUE) # January\n",
    "feb <- read.csv(\"./Data/twitter_covid_feb_small_cleaned.csv\", header = TRUE) # February\n",
    "mar <- read.csv(\"./Data/twitter_covid_mar_small_cleaned.csv\", header = TRUE) # March\n",
    "lockdown <- read.csv(\"./Data/twitter_covid_lockdown_mar2020_small_cleaned.csv\", header = TRUE) # March\n",
    "\n",
    "# Tidy\n",
    "lockdown$X.3 <- NA\n",
    "lockdown$X.2 <- NA\n",
    "lockdown$X.1 <- NA\n",
    "\n",
    "# Join together\n",
    "all_months <- rbind(jan, feb, mar, lockdown)\n",
    "#backup <- all_months # As takes ages to load!\n",
    "rm(jan, feb, mar, lockdown)\n",
    "\n",
    "## Part 1: Not true but not false misinformation ##\n",
    "\n",
    "# Misinformation lookup\n",
    "lkup <- read.csv(\"./Data/not_true_not_false.csv\", header = TRUE)\n",
    "lkup$twitter_url <-gsub(\"\\\\?amp=1.*\",\"\",lkup$twitter_url) # Remove part of url not needed\n",
    "# Save short links as lookup\n",
    "lkup_urls_short <- lkup[,c(\"twitter_url\", \"not_true_not_false\")] # Keep required vars\n",
    "lkup_urls_short <- lkup_urls_short[lkup_urls_short$twitter_url != \"\",] # Drop missing rows\n",
    "# Save full links as lookup\n",
    "lkup_urls_full <- lkup[,c(\"original_url\", \"not_true_not_false\")] # Keep required vars\n",
    "lkup_urls_full <- lkup_urls_full[lkup_urls_full$original_url != \"\",] # Drop missing rows\n",
    "# Save tweets as seperate lookup\n",
    "lkup_tw <- lkup[,c(\"tweet_id\", \"not_true_not_false\")] # Keep required vars\n",
    "lkup_tw <- lkup_tw[!is.na(lkup_tw$tweet_id),] # Drop missing rows\n",
    "rm(lkup)\n",
    "\n",
    "# Classify tweets\n",
    "\n",
    "# By Tweets\n",
    "\n",
    "# User tweets\n",
    "all_months <- merge(all_months, lkup_tw, by.x = \"id\", by.y = \"tweet_id\", all.x = TRUE) # Join on misinformation tweets lookup to main data\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$not_true_not_false_tweet1 <- all_months$not_true_not_false # Rename variable\n",
    "all_months$not_true_not_false <- NULL # Drop variable as no longer needed\n",
    "\n",
    "# Retweeted tweets\n",
    "all_months$rt_tweet_id <- as.numeric(all_months$rt_tweet_id) # Recode as numeric as stored as 'list'\n",
    "all_months <- merge(all_months, lkup_tw, by.x = \"rt_tweet_id\", by.y = \"tweet_id\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_tweet2 <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# Quoted tweets\n",
    "all_months$qt_tweet_id <- as.numeric(all_months$qt_tweet_id)\n",
    "all_months <- merge(all_months, lkup_tw, by.x = \"qt_tweet_id\", by.y = \"tweet_id\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_tweet3 <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# Combine together into single variable\n",
    "all_months$not_true_not_false_tweet <- 0\n",
    "all_months$not_true_not_false_tweet[all_months$not_true_not_false_tweet1 == 1 | \n",
    "                                      all_months$not_true_not_false_tweet2 == 1 | \n",
    "                                      all_months$not_true_not_false_tweet3 == 1] <- 1\n",
    "all_months$not_true_not_false_tweet1 <- NULL\n",
    "all_months$not_true_not_false_tweet2 <- NULL\n",
    "all_months$not_true_not_false_tweet3 <- NULL\n",
    "\n",
    "table(all_months$not_true_not_false_tweet) # n=1778\n",
    "\n",
    "# By URL Links\n",
    "\n",
    "# Links extracted from tweets\n",
    "all_months$url1 <- as.character(all_months$url1) # Change to same type (char)\n",
    "lkup_urls_short$twitter_url <- as.character(lkup_urls_short$twitter_url)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"url1\", by.y = \"twitter_url\", all.x = TRUE) # Join on lookup\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$not_true_not_false_url1 <- all_months$not_true_not_false # Rename variable\n",
    "all_months$not_true_not_false <- NULL # Drop variable as no longer needed\n",
    "\n",
    "all_months$url2 <- as.character(all_months$url2)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"url2\", by.y = \"twitter_url\", all.x = TRUE) # Repeat for other URLs\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_url2 <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "all_months$url3 <- as.character(all_months$url3)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"url3\", by.y = \"twitter_url\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_url3 <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# URLs extracted from Twitter's raw tweet information\n",
    "\n",
    "# Retweeted short URL\n",
    "lkup_urls_full$original_url <- as.character(lkup_urls_full$original_url)\n",
    "all_months$rt_url <- as.character(all_months$rt_url)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"rt_url\", by.y = \"twitter_url\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_rturl <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# Retweeted full URL\n",
    "all_months$rt_expanded_url <- as.character(all_months$rt_expanded_url)\n",
    "all_months <- merge(all_months, lkup_urls_full, by.x = \"rt_expanded_url\", by.y = \"original_url\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_rturlfull <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# Quoted short URL\n",
    "all_months$qt_url <- as.character(all_months$qt_url)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"qt_url\", by.y = \"twitter_url\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_qturl <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# Quoted full URL\n",
    "all_months$qt_expanded_url <- as.character(all_months$qt_expanded_url)\n",
    "all_months <- merge(all_months, lkup_urls_full, by.x = \"qt_expanded_url\", by.y = \"original_url\", all.x = TRUE)\n",
    "all_months$not_true_not_false[is.na(all_months$not_true_not_false)] <- 0\n",
    "all_months$not_true_not_false_qturlfull <- all_months$not_true_not_false\n",
    "all_months$not_true_not_false <- NULL\n",
    "\n",
    "# Combine into single variable\n",
    "all_months$not_true_not_false_url <- 0\n",
    "all_months$not_true_not_false_url[all_months$not_true_not_false_url1 == 1 | \n",
    "                                    all_months$not_true_not_false_url2 == 1 | \n",
    "                                    all_months$not_true_not_false_url3 == 1 | \n",
    "                                    all_months$not_true_not_false_rturl == 1 | \n",
    "                                    all_months$not_true_not_false_rturlfull == 1 | \n",
    "                                    all_months$not_true_not_false_qturl == 1 | \n",
    "                                    all_months$not_true_not_false_qturlfull == 1] <- 1\n",
    "table(all_months$not_true_not_false_url) # n=90\n",
    "all_months[35:41] <- NULL # Tidy up\n",
    "\n",
    "# Extract not true not false tweets\n",
    "ntnf_tweets <- all_months[all_months$not_true_not_false_url == 1 | all_months$not_true_not_false_tweet == 1,]\n",
    "ntnf_tweets$rt_username <- as.character(ntnf_tweets$rt_username) # Else will not save as csv\n",
    "ntnf_tweets$qt_username <- as.character(ntnf_tweets$qt_username)\n",
    "write.csv(ntnf_tweets, \"./Data/ntnf_tweets.csv\")\n",
    "\n",
    "\n",
    "## Part 2: False misinformation ##\n",
    "\n",
    "# Misinformation lookup\n",
    "lkup <- read.csv(\"./Data/false_info.csv\", header = TRUE)\n",
    "lkup$false_urls <- lkup$false_info\n",
    "lkup$twitter_url <-gsub(\"\\\\?amp=1.*\",\"\",lkup$twitter_url) # Remove part of url not needed\n",
    "# Save short links as lookup\n",
    "lkup_urls_short <- lkup[,c(\"twitter_url\", \"false_urls\")] # Keep required vars\n",
    "lkup_urls_short <- lkup_urls_short[lkup_urls_short$twitter_url != \"\",] # Drop missing rows\n",
    "# Save full links as lookup\n",
    "lkup_urls_full <- lkup[,c(\"original_url\", \"false_urls\")] # Keep required vars\n",
    "lkup_urls_full <- lkup_urls_full[lkup_urls_full$original_url != \"\",] # Drop missing rows\n",
    "# Save tweets as seperate lookup\n",
    "lkup_tw <- lkup[,c(\"tweet_id\", \"false_urls\")] # Keep required vars\n",
    "lkup_tw <- lkup_tw[!is.na(lkup_tw$tweet_id),] # Drop missing rows\n",
    "rm(lkup)\n",
    "\n",
    "# Classify tweets\n",
    "\n",
    "# By Tweets\n",
    "\n",
    "# User tweets\n",
    "all_months <- merge(all_months, lkup_tw, by.x = \"id\", by.y = \"tweet_id\", all.x = TRUE) # Join on misinformation tweets lookup to main data\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$false_urls_tweet1 <- all_months$false_urls # Rename variable\n",
    "all_months$false_urls <- NULL # Drop variable as no longer needed\n",
    "\n",
    "# Retweeted tweets\n",
    "all_months$rt_tweet_id <- as.numeric(all_months$rt_tweet_id) # Recode as numeric as stored as 'list'\n",
    "all_months <- merge(all_months, lkup_tw, by.x = \"rt_tweet_id\", by.y = \"tweet_id\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_tweet2 <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# Quoted tweets\n",
    "all_months$qt_tweet_id <- as.numeric(all_months$qt_tweet_id)\n",
    "all_months <- merge(all_months, lkup_tw, by.x = \"qt_tweet_id\", by.y = \"tweet_id\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_tweet3 <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# Combine together into single variable\n",
    "all_months$false_urls_tweet <- 0\n",
    "all_months$false_urls_tweet[all_months$false_urls_tweet1 == 1 | \n",
    "                                      all_months$false_urls_tweet2 == 1 | \n",
    "                                      all_months$false_urls_tweet3 == 1] <- 1\n",
    "all_months$false_urls_tweet1 <- NULL\n",
    "all_months$false_urls_tweet2 <- NULL\n",
    "all_months$false_urls_tweet3 <- NULL\n",
    "\n",
    "table(all_months$false_urls_tweet) # 743\n",
    "\n",
    "# By URL Links\n",
    "\n",
    "# Links extracted from tweets\n",
    "all_months$url1 <- as.character(all_months$url1) # Change to same type (char)\n",
    "lkup_urls_short$twitter_url <- as.character(lkup_urls_short$twitter_url)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"url1\", by.y = \"twitter_url\", all.x = TRUE) # Join on lookup\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$false_urls_url1 <- all_months$false_urls # Rename variable\n",
    "all_months$false_urls <- NULL # Drop variable as no longer needed\n",
    "\n",
    "all_months$url2 <- as.character(all_months$url2)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"url2\", by.y = \"twitter_url\", all.x = TRUE) # Repeat for other URLs\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_url2 <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "all_months$url3 <- as.character(all_months$url3)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"url3\", by.y = \"twitter_url\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_url3 <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# URLs extracted from Twitter's raw tweet information\n",
    "\n",
    "# Retweeted short URL\n",
    "lkup_urls_full$original_url <- as.character(lkup_urls_full$original_url)\n",
    "all_months$rt_url <- as.character(all_months$rt_url)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"rt_url\", by.y = \"twitter_url\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_rturl <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# Retweeted full URL\n",
    "all_months$rt_expanded_url <- as.character(all_months$rt_expanded_url)\n",
    "all_months <- merge(all_months, lkup_urls_full, by.x = \"rt_expanded_url\", by.y = \"original_url\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_rturlfull <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# Quoted short URL\n",
    "all_months$qt_url <- as.character(all_months$qt_url)\n",
    "all_months <- merge(all_months, lkup_urls_short, by.x = \"qt_url\", by.y = \"twitter_url\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_qturl <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# Quoted full URL\n",
    "all_months$qt_expanded_url <- as.character(all_months$qt_expanded_url)\n",
    "all_months <- merge(all_months, lkup_urls_full, by.x = \"qt_expanded_url\", by.y = \"original_url\", all.x = TRUE)\n",
    "all_months$false_urls[is.na(all_months$false_urls)] <- 0\n",
    "all_months$false_urls_qturlfull <- all_months$false_urls\n",
    "all_months$false_urls <- NULL\n",
    "\n",
    "# Combine into single variable\n",
    "all_months$false_url <- 0\n",
    "all_months$false_url[all_months$false_urls_url1 == 1 | \n",
    "                                    all_months$false_urls_url2 == 1 | \n",
    "                                    all_months$false_urls_url3 == 1 | \n",
    "                                    all_months$false_urls_rturl == 1 | \n",
    "                                    all_months$false_urls_rturlfull == 1 | \n",
    "                                    all_months$false_urls_qturl == 1 | \n",
    "                                    all_months$false_urls_qturlfull == 1] <- 1\n",
    "table(all_months$false_url) # n=208\n",
    "all_months[37:43] <- NULL # Tidy up\n",
    "\n",
    "# Extract not true not false_urls tweets\n",
    "ntnf_tweets <- all_months[all_months$false_url == 1 | all_months$false_urls_tweet == 1,]\n",
    "ntnf_tweets$rt_username <- as.character(ntnf_tweets$rt_username) # Else will not save as csv\n",
    "ntnf_tweets$qt_username <- as.character(ntnf_tweets$qt_username)\n",
    "write.csv(ntnf_tweets, \"./Data/false_tweets.csv\")\n",
    "\n",
    "\n",
    "## Part 3: Classifying URL website source ##\n",
    "\n",
    "# We can only do this via the expanded URLs for RTs and QTs unfortunately as main tweets only have t.co links saved\n",
    "\n",
    "# Load list of websites associated with fake news\n",
    "lkup_web <- read.csv(\"./Data/websites.csv\") \n",
    "lkup_web$website <- tolower(lkup_web$website) # Convert to lower case \n",
    "lkup_web$fake_website <- 1 # Add variable on\n",
    "\n",
    "# Retweets\n",
    "all_months <- merge(all_months, lkup_web, by.x = \"rt_website\", by.y = \"website\", all.x = TRUE) # Join on misinformation websites\n",
    "all_months$fake_website[is.na(all_months$fake_website)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$fake_website_rt <- all_months$fake_website # Rename variable\n",
    "all_months$fake_website <- NULL # Drop variable as no longer needed\n",
    "\n",
    "# Quoted retweets\n",
    "all_months <- merge(all_months, lkup_web, by.x = \"qt_website\", by.y = \"website\", all.x = TRUE) # Join on misinformation websites\n",
    "all_months$fake_website[is.na(all_months$fake_website)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$fake_website_qt <- all_months$fake_website # Rename variable\n",
    "all_months$fake_website <- NULL # Drop variable as no longer needed\n",
    "\n",
    "table(all_months$fake_website_rt) # n=4035\n",
    "table(all_months$fake_website_qt) # n=210\n",
    "\n",
    "# Extract tweets\n",
    "fweb_tweets <- all_months[all_months$fake_website_rt == 1 | all_months$fake_website_qt == 1,]\n",
    "fweb_tweets$rt_username <- as.character(fweb_tweets$rt_username) # Else will not save as csv\n",
    "fweb_tweets$qt_username <- as.character(fweb_tweets$qt_username)\n",
    "write.csv(fweb_tweets, \"./Data/fake_web_tweets.csv\")\n",
    "\n",
    "\n",
    "## Part 4: Identifying accounts associated with active spread of misinformation ##\n",
    "\n",
    "# Load lookup\n",
    "lkup_acc <- read.csv(\"./Data/misinformation_users.csv\", header = TRUE)\n",
    "lkup_acc$misinfo_user <- 1 # For lookup later\n",
    "\n",
    "# User tweets\n",
    "all_months <- merge(all_months, lkup_acc, by.x = \"username\", by.y = \"username\", all.x = TRUE) # Join on misinformation tweets lookup to main data\n",
    "all_months$misinfo_user[is.na(all_months$misinfo_user)] <- 0 # Recode variable as 0 if not a match\n",
    "all_months$misinfo_user_tweet <- all_months$misinfo_user # Rename variable\n",
    "all_months$misinfo_user <- NULL # Drop variable as no longer needed\n",
    "\n",
    "# Retweeted tweets\n",
    "#all_months$rt_username <- as.character(all_months$rt_username)\n",
    "all_months <- merge(all_months, lkup_acc, by.x = \"rt_username\", by.y = \"username\", all.x = TRUE)\n",
    "all_months$misinfo_user[is.na(all_months$misinfo_user)] <- 0\n",
    "all_months$misinfo_user_rt <- all_months$misinfo_user\n",
    "all_months$misinfo_user <- NULL\n",
    "\n",
    "# Quoted tweets\n",
    "all_months <- merge(all_months, lkup_acc, by.x = \"qt_username\", by.y = \"username\", all.x = TRUE)\n",
    "all_months$misinfo_user[is.na(all_months$misinfo_user)] <- 0\n",
    "all_months$misinfo_user_qt <- all_months$misinfo_user\n",
    "all_months$misinfo_user <- NULL\n",
    "\n",
    "# Combine together into single variable\n",
    "all_months$misinfo_user <- 0\n",
    "all_months$misinfo_user[all_months$misinfo_user_tweet == 1 | \n",
    "                              all_months$misinfo_user_rt == 1 | \n",
    "                              all_months$misinfo_user_qt == 1] <- 1\n",
    "all_months$misinfo_user_tweet <- NULL\n",
    "all_months$misinfo_user_rt <- NULL\n",
    "all_months$misinfo_user_qt <- NULL\n",
    "\n",
    "table(all_months$misinfo_user) # n=4905\n",
    "\n",
    "# Extract tweets\n",
    "misinfo_user_tweets <- all_months[all_months$misinfo_user == 1,]\n",
    "misinfo_user_tweets$rt_username <- as.character(misinfo_user_tweets$rt_username) # Else will not save as csv\n",
    "misinfo_user_tweets$qt_username <- as.character(misinfo_user_tweets$qt_username)\n",
    "write.csv(misinfo_user_tweets, \"./Data/misinfo_user_tweets.csv\")\n",
    "\n",
    "\n",
    "## Part 5: Identifying tweets matching WHO keywords associated with misinformation ##\n",
    "\n",
    "# Convert data to a corpus\n",
    "hold <- all_months[,c(\"id\", \"full_text_cleaned\")] # Subset required information\n",
    "# Corpus requires following column names\n",
    "names(hold)[names(hold) == \"id\"] <- \"doc_id\"\n",
    "names(hold)[names(hold) == \"full_text_cleaned\"] <- \"text\"\n",
    "corpus <- Corpus(DataframeSource(hold)) # Convert\n",
    "rm(hold)\n",
    "\n",
    "# Tidy up corpus\n",
    "corpus <- corpus %>%\n",
    "  tm_map(removePunctuation) %>% # Remove punctuation\n",
    "  #tm_map(removeNumbers) %>% # Remove numbers (not done this here as want to identify 5G)\n",
    "  tm_map(removeWords, stopwords('en')) # %>% # Remove stope words that have little meaning e.g. and, the, of etc\n",
    "#corpus <- tm_map(corpus, PlainTextDocument) \n",
    "#corpus <- Corpus(VectorSource(corpus))\n",
    "#tm_map(stripWhitespace) # Remove whitespace\n",
    "\n",
    "# Convert corpus to document term matrix\n",
    "dtm <- DocumentTermMatrix(corpus)\n",
    "rm(corpus)\n",
    "\n",
    "# Tidy data\n",
    "dtm_td <- tidy(dtm)\n",
    "rm(dtm)\n",
    "\n",
    "# Load lookup and add onto list of terms\n",
    "lkup_who <- read.csv(\"./Data/who_keywords.csv\")\n",
    "dtm_td <- merge(dtm_td, lkup_who, by.x = \"term\", by.y = \"keyword\", all.x = TRUE)\n",
    "\n",
    "# Create variables for whether match each of the types of misinformation\n",
    "dtm_td$cause <- 0\n",
    "dtm_td$cause[dtm_td$form == \"cause\"] <- 1\n",
    "dtm_td$transmission <- 0\n",
    "dtm_td$transmission[dtm_td$form == \"transmission\"] <- 1\n",
    "dtm_td$treatment <- 0\n",
    "dtm_td$treatment[dtm_td$form == \"treatment\"] <- 1\n",
    "\n",
    "# Aggregate up to tweet id so can be rejoined back onto main dataset\n",
    "dtm_td <- data.table(dtm_td)\n",
    "lkup_who_tweets <- dtm_td[, list(cause = sum(cause), transmission = sum(transmission), treatment = sum(treatment)), by = document]\n",
    "names(lkup_who_tweets)[names(lkup_who_tweets) == \"document\"] <- \"id\"\n",
    "write.csv(lkup_who_tweets, \"./Data/lookup_who_terms_tweets.csv\")\n",
    "\n",
    "\n",
    "## Part 6: Iterating through retweets of retweets ##\n",
    "\n",
    "# Taking all of the tweets that we identified as talking about misinformation, we pull out subsequent retweets of them, and then the retweets of those, and so on... To give us the cascading discussion of tweets. Snowballing data collection.\n",
    "\n",
    "# Load all misinformation lookups and and subset tweet ids\n",
    "lkup1 <- read.csv(\"./Data/ntnf_tweets.csv\") # Load\n",
    "lkup1$misinformation <- 1 # Add in required variables for consistent lookup\n",
    "lkup1$not_true_not_false <- 1 \n",
    "lkup1$false <- NA\n",
    "lkup1$cause <- NA\n",
    "lkup1$transmission <- NA\n",
    "lkup1$treatment <- NA\n",
    "lkup1 <- lkup1[,c(\"id\", \"misinformation\", \"not_true_not_false\", \"false\", \"cause\", \"transmission\", \"treatment\")] # Subset required variables\n",
    "\n",
    "lkup2 <- read.csv(\"./Data/false_tweets.csv\") # Repeat process\n",
    "lkup2$misinformation <- 1 \n",
    "lkup2$not_true_not_false <- NA\n",
    "lkup2$false <- 1\n",
    "lkup2$cause <- NA\n",
    "lkup2$transmission <- NA\n",
    "lkup2$treatment <- NA\n",
    "lkup2 <- lkup2[,c(\"id\", \"misinformation\", \"not_true_not_false\", \"false\", \"cause\", \"transmission\", \"treatment\")]\n",
    "\n",
    "lkup3 <- read.csv(\"./Data/fake_web_tweets.csv\")\n",
    "lkup3$misinformation <- 1 \n",
    "lkup3$not_true_not_false <- NA\n",
    "lkup3$false <- NA\n",
    "lkup3$cause <- NA\n",
    "lkup3$transmission <- NA\n",
    "lkup3$treatment <- NA\n",
    "lkup3 <- lkup3[,c(\"id\", \"misinformation\", \"not_true_not_false\", \"false\", \"cause\", \"transmission\", \"treatment\")]\n",
    "\n",
    "lkup4 <- read.csv(\"./Data/misinfo_user_tweets.csv\")\n",
    "lkup4$misinformation <- 1 \n",
    "lkup4$not_true_not_false <- NA\n",
    "lkup4$false <- NA\n",
    "lkup4$cause <- NA\n",
    "lkup4$transmission <- NA\n",
    "lkup4$treatment <- NA\n",
    "lkup4 <- lkup4[,c(\"id\", \"misinformation\", \"not_true_not_false\", \"false\", \"cause\", \"transmission\", \"treatment\")]\n",
    "\n",
    "lkup5a <- read.csv(\"./Data/lookup_who_terms_tweets.csv\")\n",
    "lkup5a <- lkup5a[lkup5a$cause == 1 | lkup5a$transmission == 1 | lkup5a$treatment == 1,] # Select only tweets with matches\n",
    "lkup5a[\"X\"] <- NULL # Drop variable as not needed\n",
    "lkup5a$misinformation <- 1 \n",
    "lkup5a$not_true_not_false <- NA\n",
    "lkup5a$false <- NA\n",
    "\n",
    "lkup5b <- read.csv(\"./Data/lookup_who_terms_tweets_lockdown.csv\")\n",
    "lkup5b <- lkup5b[lkup5b$cause == 1 | lkup5b$transmission == 1 | lkup5b$treatment == 1,]\n",
    "lkup5b[\"X\"] <- NULL\n",
    "lkup5b$misinformation <- 1 \n",
    "lkup5b$not_true_not_false <- NA\n",
    "lkup5b$false <- NA\n",
    "\n",
    "# Combine into one big happy list\n",
    "lkup <- rbind(lkup1, lkup2, lkup3, lkup4, lkup5a, lkup5b) # Join together\n",
    "write.csv(lkup, \"./Data/lookup_all_misinformation.csv\") # Save\n",
    "rm(lkup1, lkup2, lkup3, lkup4, lkup5a, lkup5b) # Tidy\n",
    "\n",
    "# Subset tweets that match ids\n",
    "all_months <- all_months[,c(\"id\", \"rt_tweet_id\", \"qt_tweet_id\")] # Subset required variables to make quicker\n",
    "\n",
    "lkup_rt <- merge(lkup, all_months, by.x = \"id\", by.y = \"rt_tweet_id\") # Match tweets onto lookup based on retweet ids\n",
    "lkup_rt$id <- NULL # Tidy up variables\n",
    "lkup_rt$qt_tweet_id <- NULL\n",
    "names(lkup_rt)[names(lkup_rt) == \"id.y\"] <- \"id\"\n",
    "\n",
    "lkup_qt <- merge(lkup, all_months, by.x = \"id\", by.y = \"qt_tweet_id\") # Match tweets onto lookup based on quoted retweet ids\n",
    "lkup_qt$id <- NULL \n",
    "lkup_qt$rt_tweet_id <- NULL\n",
    "names(lkup_qt)[names(lkup_qt) == \"id.y\"] <- \"id\"\n",
    "\n",
    "snowball <- rbind(lkup_rt, lkup_qt) # Join back together\n",
    "\n",
    "# Repeat process to get full cascade\n",
    "\n",
    "# 1\n",
    "lkup_rt <- merge(snowball, all_months, by.x = \"id\", by.y = \"rt_tweet_id\") \n",
    "lkup_rt$id <- NULL \n",
    "lkup_rt$qt_tweet_id <- NULL\n",
    "names(lkup_rt)[names(lkup_rt) == \"id.y\"] <- \"id\"\n",
    "\n",
    "lkup_qt <- merge(snowball, all_months, by.x = \"id\", by.y = \"qt_tweet_id\") \n",
    "lkup_qt$id <- NULL \n",
    "lkup_qt$rt_tweet_id <- NULL\n",
    "names(lkup_qt)[names(lkup_qt) == \"id.y\"] <- \"id\"\n",
    "\n",
    "hold <- rbind(lkup_rt, lkup_qt) \n",
    "\n",
    "# 2\n",
    "lkup_rt <- merge(hold, all_months, by.x = \"id\", by.y = \"rt_tweet_id\") \n",
    "lkup_rt$id <- NULL \n",
    "lkup_rt$qt_tweet_id <- NULL\n",
    "names(lkup_rt)[names(lkup_rt) == \"id.y\"] <- \"id\"\n",
    "\n",
    "lkup_qt <- merge(hold, all_months, by.x = \"id\", by.y = \"qt_tweet_id\")\n",
    "lkup_qt$id <- NULL \n",
    "lkup_qt$rt_tweet_id <- NULL\n",
    "names(lkup_qt)[names(lkup_qt) == \"id.y\"] <- \"id\"\n",
    "\n",
    "hold2 <- rbind(lkup_rt, lkup_qt) \n",
    "\n",
    "# Join all together\n",
    "snowball <- rbind(snowball, hold, hold2)\n",
    "\n",
    "# Save\n",
    "write.csv(snowball, \"./Data/lookup_snowballed_misinformation.csv\") # Save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role of bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### Identify role of bots #####\n",
    "#################################\n",
    "\n",
    "# Purpose: Explore the extent of bots in our data and what they are tweeting about.\n",
    "\n",
    "# Libraries\n",
    "library(tidytext)\n",
    "library(tm)\n",
    "library(ggplot2)\n",
    "\n",
    "## Load in Twitter data and tidy ##\n",
    "\n",
    "# Load data\n",
    "jan <- read.csv(\"./Data/twitter_covid_jan_small_cleaned.csv\", header = TRUE) # January\n",
    "jan$X.2 <- NULL # March does not have this variable so drop\n",
    "feb <- read.csv(\"./Data/twitter_covid_feb_small_cleaned.csv\", header = TRUE) # February\n",
    "feb$X.2 <- NULL\n",
    "mar <- read.csv(\"./Data/twitter_covid_mar_small_cleaned.csv\", header = TRUE) # March\n",
    "\n",
    "# Join together\n",
    "all_months <- rbind(jan, feb, mar)\n",
    "rm(jan, feb, mar)\n",
    "\n",
    "# Hold date for later\n",
    "timestamp <- all_months[,c(\"id\", \"created_at\")]\n",
    "timestamp$created_at <- as.POSIXct(timestamp$created_at, format = \"%a %b %d %H:%M:%S %z %Y\", tz = \"GMT\")  # Convert to time-date format\n",
    "\n",
    "# Convert data to a corpus\n",
    "hold <- all_months[,c(\"id\", \"full_text_cleaned\")] # Subset required information\n",
    "# Corpus requires following column names\n",
    "names(hold)[names(hold) == \"id\"] <- \"doc_id\"\n",
    "names(hold)[names(hold) == \"full_text_cleaned\"] <- \"text\"\n",
    "corpus <- Corpus(DataframeSource(hold)) # Convert\n",
    "#rm(all_months, hold)\n",
    "\n",
    "# Tidy up corpus\n",
    "corpus <- corpus %>%\n",
    "  tm_map(removePunctuation) %>% # Remove punctuation\n",
    "  #tm_map(removeNumbers) %>% # Remove numbers (not done this here as want to identify 5G)\n",
    "  tm_map(removeWords, stopwords('en')) # %>% # Remove stope words that have little meaning e.g. and, the, of etc\n",
    "\n",
    "# Convert corpus to document term matrix\n",
    "dtm <- DocumentTermMatrix(corpus)\n",
    "#rm(corpus)\n",
    "\n",
    "# Tidy data\n",
    "dtm_td <- tidy(dtm)\n",
    "#rm(dtm)\n",
    "\n",
    "\n",
    "\n",
    "## Load in users estimates of whether are bots ##\n",
    "\n",
    "# The code to generate these estimates are found in the script 'bots.R'. It will not run on this machine as I had trouble installing devtools. I ran it locally instead. I am unsure over the quality of the results, but it provides a good starting point for us. There is some missing data in it where the code was unable to estimate (likely as account had been removed - so a good chance NAs are bots).\n",
    "bots <- read.csv(\"./Data/predict_bots.csv\") # Load in estimates of whether bot or not\n",
    "\n",
    "# Let's have a quick look at this data\n",
    "ggplot(bots,aes(prob_bot)) +\n",
    "  geom_histogram() +\n",
    "  xlab(\"Probability user is a bot\") +\n",
    "  ylab(\"Frequency\")\n",
    "\n",
    "# Define bot or not\n",
    "# Since we get a probability rather than a binary classification, we need to define a cut off point for whether we estimate a user to be a bot or not. There is no guidance by the package creator on this figure, but I have seen both 0.7 and 0.5 discussed across the internet. This is something to review. For the example code here, let's go for 0.5 for now (~33.6% users defined as bots).\n",
    "bots$bot <- NA\n",
    "bots$bot[bots$prob_bot > 0.5] <- 1\n",
    "bots$bot[bots$prob_bot <= 0.5] <- 0\n",
    "\n",
    "## Examine trends in bots posting ##\n",
    "\n",
    "# Join bots onto twitter dataset\n",
    "all_months <- merge(all_months, bots, by = \"user_id\", all.x = TRUE)\n",
    "all_months$X.y <- NULL # Delete repeated columns\n",
    "all_months$screen_name <- NULL\n",
    "\n",
    "# Aggregate counts by bot or not per day\n",
    "all_months$created_at <- as.POSIXct(all_months$created_at, format = \"%a %b %d %H:%M:%S %z %Y\", tz = \"GMT\") # Convert to time-date format\n",
    "all_months$day <- cut(all_months$created_at, breaks=\"day\") # Split by day\n",
    "all_months$freq <- 1 # To make next step easier\n",
    "day_sum <- aggregate(freq~day+bot, all_months, sum) # Count number of bots or not per day\n",
    "day_sum$day <- as.Date(day_sum$day, format='%Y-%m-%d') # Convert to date (as factor)\n",
    "\n",
    "# Summary statistics\n",
    "tab <- aggregate(freq~bot, day_sum, sum) # Sum number of tweets over period by bot or not\n",
    "tab$percent <- (tab$freq / sum(tab$freq))*100 # Calculate percentage by bot (1) or not (0)\n",
    "tab # So 42% of all tweets from bots but only 33% of users\n",
    "\n",
    "# Plot\n",
    "day_sum %>%\n",
    "  ggplot(aes(x = day, y = freq, group = as.factor(bot), color = as.factor(bot))) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method = \"gam\", se=F) + # Add smoothed line on to summarise trend (GAM is better for memory with larger datasets compared to LOESS)\n",
    "  scale_color_discrete(name = NULL, labels = c(\"Human\", \"Bot\", \"C\")) + # Add labels to plot\n",
    "  ylab(\"Frequency\") +\n",
    "  xlab(\"Day\")\n",
    "\n",
    "# We might want to consider better ways of displaying the data, such as logging the frequency so that larger counts do not swamp earlier patterns:\n",
    "day_sum$log_freq <- log(day_sum$freq)\n",
    "day_sum %>%\n",
    "  ggplot(aes(x = day, y = log_freq, group = as.factor(bot), color = as.factor(bot))) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method = \"gam\", se=F) + \n",
    "  scale_color_discrete(name = NULL, labels = c(\"Human\", \"Bot\", \"C\")) + \n",
    "  ylab(\"Logged count\") +\n",
    "  xlab(\"Day\")\n",
    "\n",
    "# What terms are used by bots and not\n",
    "\n",
    "# So can reproduce plot\n",
    "set.seed(250388)\n",
    "\n",
    "# Join on bots classification\n",
    "id_lkup <- all_months[,c(\"id\", \"user_id\")] # Create lookup of tweet id and user id\n",
    "id_lkup <- merge(id_lkup, bots, by = \"user_id\", all.x = TRUE) # Join on bots lkup\n",
    "dtm_td <- merge(dtm_td, id_lkup, by.x = \"document\", by.y = \"id\", all.x = TRUE)\n",
    "\n",
    "# Aggregate words\n",
    "dtm_td_agg <- dtm_td %>%\n",
    "  group_by(term, bot) %>%\n",
    "  summarise(count = sum(count))\n",
    "\n",
    "# Most common words\n",
    "dtm_td[!is.na(dtm_td$bot),] %>%\n",
    "  count(term, bot, sort = TRUE) %>%\n",
    "  group_by(bot) %>%\n",
    "  top_n(20) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(term = reorder(term, n)) %>%\n",
    "  ggplot(aes(term, n, fill = factor(bot))) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~bot, scales = \"free_y\") +\n",
    "  labs(y = \"Term frequency\",\n",
    "       x = NULL) +\n",
    "  coord_flip()\n",
    "\n",
    "# Plot wordcloud comparing terms they differ on\n",
    "dtm_td[!is.na(dtm_td$bot),] %>%\n",
    "  count(term, bot, sort = TRUE) %>%\n",
    "  acast(term ~ bot, value.var = \"n\", fill = 0) %>%\n",
    "  comparison.cloud(colors = c(\"gray20\", \"gray80\"),\n",
    "                   max.words = 100)\n",
    "\n",
    "# We could also compare term frequency by a scatter plot approach\n",
    "# Tidy data\n",
    "terms_spread <- dtm_td[!is.na(dtm_td$bot),] %>%\n",
    "  count(term, bot, sort = TRUE) %>%\n",
    "  spread(bot, n)\n",
    "names(terms_spread)[names(terms_spread) == \"1\"] <- \"bot\"\n",
    "names(terms_spread)[names(terms_spread) == \"0\"] <- \"human\"\n",
    "terms_spread$human[is.na(terms_spread$human)] <- 0 # Add in missing data as 0\n",
    "\n",
    "# Plot\n",
    "terms_spread %>% \n",
    "  ggplot(aes(x = human, y = bot)) + # Scatter plot of terms across topics 1 and 2\n",
    "  geom_point(alpha = 0.1) +\n",
    "  coord_cartesian(xlim = c(0,250000), ylim = c(0,250000)) + # Don't display outliers\n",
    "  xlab(\"Human Term Frequency\") +\n",
    "  ylab(\"Bot Term Frequency\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
